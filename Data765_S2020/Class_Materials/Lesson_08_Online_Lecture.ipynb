{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font face=\"times\"><font size=\"6pt\"><p style = 'text-align: center;'> The City University of New York, Queens College\n",
    "\n",
    "<font face=\"times\"><font size=\"6pt\"><p style = 'text-align: center;'><b>Introduction to Computational Social Science</b><br/><br/>\n",
    "\n",
    "<p style = 'text-align: center;'><font face=\"times\"><b>Lesson 08 | PCA and K-Means Clustering</b><br/><br/>\n",
    "\n",
    "<p style = 'text-align: center;'><font face=\"times\"><b>2 Checkpoints</b><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60\n",
    "\n",
    "Source: https://towardsdatascience.com/k-means-clustering-with-scikit-learn-6b47a369a83c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "# Lesson 08\n",
    "# PCA and K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "# Unsupervised Learning: Principle Component Analysis\n",
    "\n",
    "Here we'll briefly go into a bit of depth on an important unsupervised learning technique, **Principal Component Analysis (PCA)**.\n",
    "\n",
    "By the end of this section you should\n",
    "\n",
    "- be able to describe how PCA reduces dimensionality\n",
    "- see how these can be applied to several interesting problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Component Analysis is a very powerful **unsupervised** method for *dimensionality reduction* in data.\n",
    "\n",
    "Using PCA for dimensionality reduction involves zeroing out one or more of the smallest \"principle components,\"\n",
    "resulting in a lower-dimensional projection of the data that preserves the maximal data variance.\n",
    "\n",
    "In other words, PCA is a technique that finds the underlying variables (known as principal components) that best differentiates your data points. In other words, these principal components are dimensions along which your data points are the MOST spread out. \n",
    "\n",
    "![Visualizing the PCA](Images/11_PCA.png)\n",
    "\n",
    "\n",
    "PCA is called a \"dimension reducing\" technique because it \"linearlly combines\" variables that tend to be highly correlated with one another. This isn't a problem if you're only looking at four or five variables.\n",
    "\n",
    "However, if you're looking at hundreds (or even thousands) of variables, PCA can collapse them into a handful of \"mega\" variables that capture all of the variance that these hundreds of varaibles were doing individually. In other words, a principal component is a weighted \"average\" of all these hundreds of variables. \n",
    "\n",
    "For instance, consider a simple data set that looks at four variables for fruits and vegetables:\n",
    "\n",
    "- Fat\n",
    "- Protein\n",
    "- Fiber\n",
    "- Vitamin C\n",
    "\n",
    "These four variables describe the nutritional make-up of various foods (e.g., lamb, pork, kale, parsley).\n",
    "\n",
    "![Food Variance](Images/11_PCA_Food_Variable_Example.png)\n",
    "\n",
    "As we can see, lamb and pork are both high in fat and protein, while kale and parsley are high in fiber and vitamin C. (They're good for you!) So there's something about the features of these various foods that are shared by their wider categories (i.e., meats, vegetables, legumes, etc.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use PCA, as an unsuperivsed method, to see how these different foods cluster together based on their features with no assumptions or background knowledge about them! So, if we were to run a PCA on these data (let's say we're interested in four principal components), we could get the following: \n",
    "\n",
    "![PCA Table](Images/11_PCA_Table_Example.png)\n",
    "\n",
    "\n",
    "The rows were the original variables, and the columns are the \"new\" (component) variables, or weighted combinations of the old ones. By convention, the first principal component explains the most variance, followed by the second, and so forth. In fact, we can plot how much of the spread of the data is explained by each component. Let's plot this for each of our four components. \n",
    "\n",
    "\n",
    "![Variance Explained](Images/11_PCA_Variance_Explained.png)\n",
    "\n",
    "For visualization purposes, we should only consider the fewest number of principal components that explain the most spread (i.e., the most bang of our buck). There's not true-and-tried rule for selecting the number of components. We often look for a \"kink\" or \"bow\" in the above plot. This usually indicates diminishing returns to each additional value of K. As the plot above shows, more than 2 principal components offers little value add. In other words, the first two components capture about 45% (PC1) + 25% (PC2) = 70% of the spread of all of our data. Not bad!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returning to our original table, let's look at the first two components, PC1 and PC2, as they explain a good amount of the variance for us. Think of each component as an \"axis,\" like the y-axis or x-axis. For PC1, we can see that fiber and vitamin C are closely aligned together in the positive region of the axis, whle fat and protein are closely aligned on their end of the axis. For PC2, we see fat and vitamin C are closely aligned, whle protein and fiber are clustered together. \n",
    "\n",
    "What does this all mean? These features of the principal components are in-fact CLUSTERING different types of foods together by their nutritional characteristics. If we plot our hypothetical data points on PC1 (say as our x-axis) and PC2 (as our y-axis) for various foods we see an interesting trend. \n",
    "\n",
    "![PCA Plot](Images/11_PCA_Plotting_PC1_and_PC2.png)\n",
    "\n",
    "Meats and fish are clustered to the left of the plot, while vegetables are legumes are clustered to the right. This makes sense: meat and fish are high in fat and protein, so they're to the left in PC1 (negative x-axis), while vegetables and legumes are high in fiber and vitamin C (positive x-axis). (Note: the negative or positive values DOES NOT indicates where values are clustered on the principal component axis, not that it has less of something, necessarily.) Similarly, for PC2, we see high-fat meats near the top (e.g., lamb, pork, beef) and vegetables high in vitamin C also at the top (e.g., kale, parsley).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read in some data from UC, Irvine's machine learning database. These data capture various physical features of a flower. We'll run a PCA to see if we can collapse multiple features that describe these data into only a few dimensions. \n",
    "\n",
    "Note: You'll need internet access to get these data. First, load dataset into a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "\n",
    "df = pd.read_csv(url, names=['sepal length','sepal width','petal length','petal width','target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## Standardize the Data\n",
    "\n",
    "Often, many of the variables that we are interested in using are on different scales (e.g., one variable might vary from 1 to 1,000, while another may include just five values). PCA is effected by scale so you need to scale the features in your data before applying PCA. \n",
    "\n",
    "Thus, just as we did before with the ANN, we need to **standardize** these varibles. This usually means centering these varibles (i.e., or substraing the mean of each variable for all of its data points) and then creating some common unit (i.e., usually achieved by then dividing this value by its standard deviation, so that all of your variables are comparable in terms of standard deviations). In other words, all of our variables will have mean = 0 and variance = 1. \n",
    " \n",
    "Use ***StandardScaler*** to help you standardize the dataset’s features onto unit scale which is a requirement for the optimal performance of many machine learning algorithms. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at four features of a flower: the length and width of a sepal, and the length and width of its petals. In other words, let's subset these data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = df[['sepal length', 'sepal width', 'petal length', 'petal width']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column \"target\" in the original dataframe tells us what type of flower it is. So, using these characteristics, we can see if these features properly explain the most variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = df[['target']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's standardize these features such that means are all set to zero and their unit-standardized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = StandardScaler().fit_transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Standardizing Dataset](Images/11_Petal_Standardized.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original data has 4 columns (i.e., sepal length, sepal width, petal length, and petal width). Here, we'll project the original data which is 4 dimensional into 2 dimensions. \n",
    "\n",
    "As we saw before, there's usually a particular meaning assigned to each principal component. And the new components are just the two main dimensions of variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2) #Set up the model, and we only want to collapse these data into two dimensions. \n",
    "principalComponents = pca.fit_transform(x) #Take our data--saved as x--and run the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now let's take the results--saved as principalComponents--and save it to a DataFrame.\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['principal component 1', 'principal component 2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![PCA of Data](Images/11_petal_pca_df.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "finalDf = pd.concat([principalDf, df[['target']]], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's concatenate the DataFrame along its index, where finalDf is the final DataFrame before plotting these data. Let's take a look. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "finalDf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's plot these two components. \n",
    "\n",
    "First let's input some plotting packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's plot these data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('2 component PCA', fontsize = 20)\n",
    "targets = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\n",
    "colors = ['r', 'g', 'b']\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = finalDf['target'] == target\n",
    "    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n",
    "               , finalDf.loc[indicesToKeep, 'principal component 2']\n",
    "               , c = color\n",
    "               , s = 50)\n",
    "ax.legend(targets)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you notice about the above plot? The PCA, using the aforementioned features, was able to split our data rather cleanly into different groups! Notice, that we did not train or test our data, because PCA is an unsupervised method (no training necessary!). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explained Variance\n",
    "\n",
    "The explained variance tells you how much information (variance) can be attributed to each of the principal components. This is important (as we discussed earlier) because, while you can convert 4 dimensional space to 2 dimensional space, you do lose some of the variance (information) when you do this. \n",
    "\n",
    "By using the attribute explained_variance_ratio_, you can see that the first principal component contains 72.77% of the variance and the second principal component contains 23.03% of the variance. Together, the two components contain 95.80% of the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this tell you? The first component explaines 72% of the variance, while the second explains 23%. That's a total of 95%! In other words, the first two components explains a lot of the variance of the data! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that scikit-learn contains many other unsupervised dimensionality reduction routines: some you might wish to try are\n",
    "Other dimensionality reduction techniques which are useful to know about:\n",
    "\n",
    "- [sklearn.decomposition.PCA](http://scikit-learn.org/0.13/modules/generated/sklearn.decomposition.PCA.html): \n",
    "   Principle Component Analysis\n",
    "- [sklearn.decomposition.RandomizedPCA](http://scikit-learn.org/0.13/modules/generated/sklearn.decomposition.RandomizedPCA.html):\n",
    "   fast non-exact PCA implementation based on a randomized algorithm\n",
    "- [sklearn.decomposition.SparsePCA](http://scikit-learn.org/0.13/modules/generated/sklearn.decomposition.SparsePCA.html):\n",
    "   PCA variant including L1 penalty for sparsity\n",
    "- [sklearn.decomposition.FastICA](http://scikit-learn.org/0.13/modules/generated/sklearn.decomposition.FastICA.html):\n",
    "   Independent Component Analysis\n",
    "- [sklearn.decomposition.NMF](http://scikit-learn.org/0.13/modules/generated/sklearn.decomposition.NMF.html):\n",
    "   non-negative matrix factorization\n",
    "- [sklearn.manifold.LocallyLinearEmbedding](http://scikit-learn.org/0.13/modules/generated/sklearn.manifold.LocallyLinearEmbedding.html):\n",
    "   nonlinear manifold learning technique based on local neighborhood geometry\n",
    "- [sklearn.manifold.IsoMap](http://scikit-learn.org/0.13/modules/generated/sklearn.manifold.Isomap.html):\n",
    "   nonlinear manifold learning technique based on a sparse graph algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "# Checkpoint 1 of 2\n",
    "## Now you try! \n",
    "\n",
    "### Read in the wine data that we've used before. \n",
    "### Run a PCA on these data, following the same procedures as above (i.e., standardizing variables, etc.). First select three-to-four features (i.e., columns). \n",
    "\n",
    "### Run your PCA using just two components. How much of the explained variance do you capture with two components?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in the wine data.\n",
    "import pandas as pd\n",
    "wine = pd.read_csv('Data/wine_data.csv', \n",
    "                   names = [\"Cultivator\", \n",
    "                            \"Alchol\", \n",
    "                            \"Malic_Acid\", \n",
    "                            \"Ash\", \n",
    "                            \"Alcalinity_of_Ash\", \n",
    "                            \"Magnesium\", \n",
    "                            \"Total_phenols\", \n",
    "                            \"Falvanoids\", \n",
    "                            \"Nonflavanoid_phenols\", \n",
    "                            \"Proanthocyanins\", \n",
    "                            \"Color_intensity\", \n",
    "                            \"Hue\", \n",
    "                            \"OD280\", \n",
    "                            \"Proline\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "# K-Means Clustering\n",
    "\n",
    "Clustering (or cluster analysis) is a technique that allows us to find groups of similar objects, objects that are more related to each other than to objects in other groups. \n",
    "\n",
    "Examples of business-oriented applications of clustering include:\n",
    "  * The grouping of documents, music, and movies by different topics\n",
    "  * Finding customers that share similar interests based on common purchase behaviors as a basis for recommendation engines.\n",
    "\n",
    "The k-means algorithm is pretty easy to implement and is also  very efficient compared to other clustering algorithms, which might explain its popularity. \n",
    "\n",
    "The k-means algorithm belongs to the category of prototype-based clustering. Prototype-based clustering means that each cluster is represented by a prototype, which can either be the **centroid** (average) of similar points for continuous features, or in the case of categorical features, the **medoid** (i.e., the most representative or most frequently occurring point). \n",
    "\n",
    "Below is an example of three **centroids** identifying three clusters of data in some x and y-axis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![KMeans of Data](Images/09_image_k_means_clustering.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's again use `sklearn` to import `KMeans`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the preceding code, we set the number of desired clusters to 3. \n",
    "\n",
    "We set `n_init`=10 to run the k-means clustering algorithms 10 times independently with different random centroids to choose the final model as the one with the lowest SSE (sum-of-squared errors). \n",
    "\n",
    "Via the `max_iter` parameter, we specify the maximum number of iterations for each single run (here, 300).\n",
    "\n",
    "**Note:** The k-means implementation in `scikit-learn` stops early if it converges before the maximum number of iterations is reached. However, it is possible that k-means does not reach convergence for a particular run, which can be problematic (computationally expensive) if we choose relatively large values for `max_iter`.\n",
    "\n",
    "One way to deal with convergence problems is to choose larger values for tol, which is a parameter that controls the tolerance with regard to the changes in the within-cluster sum-squared-error to declare convergence. \n",
    "\n",
    "In the preceding code, we chose a `tolerance` of 1e-04 (= 0.0001), denoted by `tol`. \n",
    "\n",
    "A problem with k-means is that one or more clusters can be empty. However, this problem is accounted for in the current k-means implementation in scikit-learn. If a cluster is empty, the algorithm will search for the sample that is farthest away from the centroid of the empty cluster. Then it will reassign the centroid to be this farthest point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "km = KMeans(\n",
    "    n_clusters=2, \n",
    "    init='random',\n",
    "    n_init=10, \n",
    "    max_iter=300, \n",
    "    tol=1e-04, \n",
    "    random_state=0\n",
    ")\n",
    "y_km = km.fit_predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have predicted the cluster labels `y_km`, let’s visualize the clusters that k-means identified in the dataset together with the cluster centroids. \n",
    "\n",
    "These are stored under the `cluster_centers_` attribute of the fitted `KMeans` object, which we call `km` here. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Just in case you hadn't used it earlier.\n",
    "%matplotlib inline \n",
    "\n",
    "# plot the two clusters\n",
    "# Cluster 1\n",
    "plt.scatter(\n",
    "    x[y_km == 0, 0], x[y_km == 0, 1],\n",
    "    s=50, \n",
    "    c='lightgreen',\n",
    "    marker='s', edgecolor='black',\n",
    "    label='cluster 1'\n",
    ")\n",
    "\n",
    "# Cluster 2\n",
    "plt.scatter(\n",
    "    x[y_km == 1, 0], x[y_km == 1, 1],\n",
    "    s=50, c='orange',\n",
    "    marker='o', edgecolor='black',\n",
    "    label='cluster 2'\n",
    ")\n",
    "\n",
    "\n",
    "# Plot the centroids\n",
    "plt.scatter(\n",
    "    km.cluster_centers_[:, 0], km.cluster_centers_[:, 1],\n",
    "    s=250, marker='*',\n",
    "    c='red', edgecolor='black',\n",
    "    label='centroids'\n",
    ")\n",
    "plt.legend(scatterpoints=1)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "How do we pick the right value of `k`?\n",
    "\n",
    "The **elbow method** is a useful graphical tool to estimate the optimal number of clusters k for a given task. \n",
    "\n",
    "Intuitively, we can say that, if `k` increases, the within-cluster SSE (“**distortion**”) will decrease. This is because the samples will be closer to the centroids they are assigned to.\n",
    "\n",
    "In other words, the model clusters data by trying to separate samples in `n` groups of equal variances, minimizing a criterion known as `inertia`, or within-cluster sum-of-squares error. Inertia is roughly a measure of how internally coherent clusters are. This is provided within the `km` object, using `.intertia_`. \n",
    "\n",
    "The idea behind the elbow method is to identify the value of `k` where the distortion begins to decrease most rapidly, which will become clearer if we plot the distortion for different values of `k`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "distortions = [] #Create a list to store the distortion values for each value of k, ranging from 1 to 10\n",
    "for i in range(1, 11):\n",
    "    km = KMeans(\n",
    "        n_clusters=i, # each value of k is between 1 and 10 (recall that in range(), the max value is max+1)\n",
    "        init='random',\n",
    "        n_init=10, \n",
    "        max_iter=300,\n",
    "        tol=1e-04, \n",
    "        random_state=0\n",
    "    )\n",
    "    km.fit(x)\n",
    "    distortions.append(km.inertia_) # append the distortion value, using the value from .inertia_\n",
    "\n",
    "# plot\n",
    "plt.plot(range(1, 11), distortions, marker='o')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Distortion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***\n",
    "***\n",
    "\n",
    "# Checkpoint 2 of 2\n",
    "## Now you try! \n",
    "\n",
    "### Re-read in the wine data that we've used before (just in case if you manipulated the `wine` `DataFrame` with the earlier PCA).\n",
    "### Now, run a k-means model on these data. Again, don't forget to standardize your variables. Use the same  three-to-four features (i.e., columns) you used in the first checkpoint. \n",
    "\n",
    "### Run your k-means model using just two centroids/medoids. \n",
    "\n",
    "### Plot these two clusters and visually inspect them. How well does your k-means model fit your data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
