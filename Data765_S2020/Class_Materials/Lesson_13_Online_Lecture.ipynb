{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font face=\"times\"><font size=\"6pt\"><p style = 'text-align: center;'> The City University of New York, Queens College\n",
    "\n",
    "<font face=\"times\"><font size=\"6pt\"><p style = 'text-align: center;'><b>Introduction to Computational Social Science</b><br/><br/>\n",
    "\n",
    "<p style = 'text-align: center;'><font face=\"times\"><b>Lesson 13 | Social Network Analysis II</b><br/><br/>\n",
    "\n",
    "<p style = 'text-align: center;'><font face=\"times\"><b>8 Checkpoints</b><br/><br/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "# Begin Lesson 13\n",
    "## Social Network Analysis\n",
    "\n",
    "This week, we're going dive into the meat of social network analysis. There is a lot that we can't cover, so we're going to focus on two big questions surrounding social networks:\n",
    "\n",
    "1. How you can tell whether a node is \"important\" (this is what centrality is all about)\n",
    "2. How you can identify \"groups\" in a network (we call them \"communities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## Centrality Measures\n",
    "\n",
    "![Neuron](Images/13_Network_Centrality_Measures.png) \n",
    "\n",
    "Centrality measures are used to determine what structural features are afforded to nodes. In other words, we can look at where nodes are in the network and see what advantages or disadvantages they have as a result of how they are situated with respect to everyone else. Similarly we can use centrality measures to figure out which are the most important nodes for this or that. For instance, someone who is the sole \"bridge\" between two insular and seperate groups of the network acts as a gatekeeper of knowledge, where all knowledge about the other side needs to go through them first (think spies!).\n",
    "\n",
    "There are several different centrality measures that can each tell us something different about the node's position. The image above shows three popular centrality meausres - (1) Betweenness; (2) Closeness; (3) and Eigenvector - and the nodes that have the highest values for each. We've actually already covered \"Degree\" centrality last week (i.e., total number of edges) so this should be somewhat familiar to you. \n",
    "\n",
    "These measures aren't necessarily mutually exclusive: a node can have have eigenvector and high degree centrality, for instance. However, this doesn't always hold (e.g., it's hard to have high betweeness centrality and high degree centrality).\n",
    "\n",
    "We will only be talking about centrality measures in the context of undirected graphs. Be aware that the calculation and meaning of these measures changes in directed graphs, much like how degree needs to be measured as in-degree and out-degree in a directed graph.\n",
    "\n",
    "Let's use the `Hartford` data again. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hartford = nx.read_edgelist('Data/hartford_drug.txt', # Text file saved as an edgelist with a sender, receive column\n",
    "                            create_using=nx.DiGraph(), # Set it up as a \"Directed\" graph\n",
    "                            nodetype=int) # The nodes don't have labels, they are integers used to mask people's actually names. \n",
    "hartford_ud = hartford.to_undirected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hartford_ud_components = [g for g in nx.connected_component_subgraphs(hartford_ud)][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "# Checkpoint 1 of 8\n",
    "## Now you try!\n",
    "\n",
    "### Read in and set up the Star Wars data from last time. (Don't  use `read_edgelist()`, as this works better for unweighted networks. Call it `SW_G`. \n",
    "\n",
    "### Calculate the components of your network, `SW_G`. Save it as `sw_components`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "### Betweenness centrality\n",
    "\n",
    "![Neuron](Images/13_Network_Betweeness.png) \n",
    "\n",
    "Betweenness centrality is based on communication flow. For instance, consider the hypothetical social network above of several friends. Nodes with a high betweenness centrality (like \"Liz\" in our social network here) are interesting because they lie on communication paths and can control information flows. In other words, nodes with high betweenness centrality are \"between\" a lot of information flow between all other nodes in the network. \n",
    "\n",
    "To calculate this, we can use the function `betweenness_centrality()`. Let's do this for the hartford data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Betweenness centrality\n",
    "bet_cen = nx.betweenness_centrality(hartford_ud_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's get the maximum betweenness centrality in the network\n",
    "print(max(bet_cen.values()))\n",
    "print(min(bet_cen.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "# Checkpoint 2 of 8\n",
    "## Now you try!\n",
    "\n",
    "### Calculate the betweenness centrality. Which nodes have the highest and lowest values? \n",
    "\n",
    "### For you Star Wars fans, does this make sense?\n",
    "\n",
    "### For you non-Star Wars fans, recall the plot from last time. Does this make sense?\n",
    "### **Hint:** Include the argument `weight` to indicate that this is a weighted network. For instance: \n",
    "\n",
    "                nx.betweenness_centrality(YOUR_NETWORK_HERE,weight='weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "### Closeness centrality\n",
    "\n",
    "![Neuron](Images/13_Network_Closeness.png) \n",
    "\n",
    "Closeness centrality is the average distance between one node and the others in a network. Put in more technical language, it is the average of the length of all shortest paths between the node in question to all other nodes in the network.\n",
    "\n",
    "It's a measure of \"reach,\" or how fast information can flow to all other nodes from this node. So, as compared to other people in this same network, \"Emma\" is (on average) the \"closest\" to everyone else. She can easily spread information in the network.\n",
    "\n",
    "To calculate this, we use `closeness_centrality()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Closeness centrality\n",
    "clo_cen = nx.closeness_centrality(hartford_ud_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Again let's get the maximum and minimum\n",
    "print(max(clo_cen.values()))\n",
    "print(min(clo_cen.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "# Checkpoint 3 of 8\n",
    "## Now you try!\n",
    "\n",
    "### Calculate the closeness centrality. Again, which nodes have the highest and lowest values? \n",
    "\n",
    "### For you Star Wars fans, does this make sense?\n",
    "\n",
    "### For you non-Star Wars fans, recall the plot from last time. Does this make sense?\n",
    "\n",
    "### **Hint:** Include the argument `weight` to indicate that this is a weighted network. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "### Eigenvector centrality\n",
    "\n",
    "![Neuron](Images/13_Network_Eigenvector.png) \n",
    "\n",
    "Eigenvector centrality is the most abstract to think of, but its arguably one of the most important and applicable. Think of it as a measure of how important your neighbors are. The intuition here is that a person (or website, or scholarly article, or politician) is important when they are connected to important people (or websites, or scholarly articles, or politicians, respectively).\n",
    "\n",
    "For instance, consider this new network above. The node in yellow has the highest eigenvector centrality, because its connected to the two most popular nodes in blue. Note, that the nodes in blue are well connected, they're not connected to the other popular blue nodes.\n",
    "\n",
    "This is how Google's PageRank algorithm works: if you're a blogger that the New York Times and CNN links to and cites, then you have a high eigenvector score, even if not that many other people link to your site. Think of it like a \"reputation score.\" Webpages (like blogs) that have higher eigenvector scores are more likely to show up as one of your top search results than a webpage that just has a lot of non-important links. \n",
    "\n",
    "To calculate this, use the function `eigenvector_centrality()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Eigenvector centrality\n",
    "eig_cen = nx.eigenvector_centrality(hartford_ud_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Max and min again\n",
    "print(max(eig_cen.values()))\n",
    "print(min(eig_cen.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "# Checkpoint 4 of 8\n",
    "## Now you try!\n",
    "\n",
    "### Calculate the eigenvector centrality. Again, which nodes have the highest and lowest values? \n",
    "\n",
    "### For you Star Wars fans, does this make sense?\n",
    "\n",
    "### For you non-Star Wars fans, recall the plot from last time. Does this make sense?\n",
    "\n",
    "### **Hint:** Include the argument `weight` to indicate that this is a weighted network. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## Bipartite Networks\n",
    "\n",
    "![Neuron](Images/13_Network_Bipartite.png) \n",
    "\n",
    "\n",
    "\n",
    "So far, we've only looked at networks with different types of edges-- directed, weighted, multiplex. We have yet to look at networks that have different types of nodes. For instance, consider the network shown above. It is a network comprised of two different types of nodes. You could think of them as people in the class and their various social affliations, like the high school they attended, college you currently attend, city you grew up in, where you work, etc. Bipartite networks are particulary helpful at understanding membership in groups (e.g., how people know one another from having the same membership, what potential advantages or disadvantages they may have due to these memberships, etc.). \n",
    "\n",
    "The bipartite algorithms are not imported into the NetworkX namespace at the top level so the easiest way to use them is with `bipartite` from `networkx.algorithms`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from networkx.algorithms import bipartite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first learn how to creat a bipartite graph from scratch, just let we did with what we call unipartite graphs (i.e., networks with just one type of node, which is what we've been dealing with exclusively so far). \n",
    "\n",
    "`NetworkX` doesn't have an explicity bipartite graph, so we'll have to define one from the basic `Graph()` object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "B = nx.Graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add nodes with the node attribute \"bipartite.\" Let's also base it on the graph we see above, so let's add persons \"a\",\"b\",\"c\",\"d\", and \"e\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "B.add_nodes_from(['a', 'b', 'c','d','e'], \n",
    "                 bipartite=0) #We set bipartite to be 0, the first class of nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "B.add_nodes_from([1, 2, 3, 4], \n",
    "                 bipartite=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, add edges only between nodes of opposite node sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "B.add_edges_from([('a',1), ('a',2), \n",
    "                  ('b',2), ('b',3),\n",
    "                  ('c',1), ('c',3),\n",
    "                  ('d',2),('d',4),\n",
    "                  ('e',3),('e',4) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, Y = bipartite.sets(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos = dict()\n",
    "pos.update( (n, (1, i)) for i, n in enumerate(X) ) # put nodes from X at x=1\n",
    "pos.update( (n, (2, i)) for i, n in enumerate(Y) ) # put nodes from Y at x=2\n",
    "nx.draw(B, pos=pos)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "# Checkpoint 5 of 8\n",
    "## Now you try!\n",
    "\n",
    "### Create your own bipartite network! \n",
    "\n",
    "### Create one set of nodes, call them: `x`, `y`, and `z`. These are the senders. \n",
    "### Create a second set of nodes, call them: `10`, `11`, `12`, and `13`. These are the receivers. \n",
    "### Call your new bipartite network `B2`. \n",
    "\n",
    "### Use the method `.add_edges_from()` and add random edges going from the senders to the receivers. Create at least six links from the senders to the receivers. \n",
    "\n",
    "### Draw `B2`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## Real-world Bipartite Data. \n",
    "\n",
    "Let's now use some real-world data. \n",
    "\n",
    "We'll use as an example the data collected by Davis et al. in 1930s about the observed attendance at 14 social events by 18 women in a Southern state of the USA.\n",
    "\n",
    "The nodes in the bipartite graph are both women and events, and each women is linked to the events that she attended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D = nx.davis_southern_women_graph()\n",
    "list(D.nodes(data=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centrality measures of bipartite graphs\n",
    "\n",
    "In order to compute centrality measures for bipartite graphs, we cannot use the same algorithms that we used for unipartite graphs. This is because how these measures are normalized is different. \n",
    "\n",
    "For instance, the degree centrality of a node is defined as the degree of a node divided by the maximum possible degree. In unipartite networks, the maximum degree of a node is $n-1$, where $n$ is the total number of nodes of a graph. However, in a bipartite graph, a node's maximum degree is only the total number of nodes in the opposite set. In other words, the maximum degree for a woman in our graph is the number of events, not the number of women.\n",
    "\n",
    "`NetworkX` provides functions to compute centrality measures for bipartite graphs.\n",
    "\n",
    "In order to use these functions you have to pass as an argument a set with all nodes in one bipartite set. We'll need `itemgetter` to extract out the values for each key-value pair in the dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get the degree, betweenness, and closeness centralities for the bipartite network. First we have to make a set of nodes to indicate which are women, and which are events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "women = [n for n, d in D.nodes(data=True) if d['bipartite']==0]\n",
    "events = [n for n, d  in D.nodes(data=True) if d['bipartite']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "degc = bipartite.degree_centrality(D, women)\n",
    "sorted(degc.items(), key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bet = bipartite.betweenness_centrality(D, women)\n",
    "sorted(bet.items(), key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clos = bipartite.closeness_centrality(D, women)\n",
    "sorted(clos.items(), key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you notice about the centrality scores for both women and the events attended? How are they similar? How are they different?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## Projection from bipartite to unipartite\n",
    "\n",
    "\n",
    "The process of obtaining an unipartite graph (i.e., with only women that are linked if they attended the same events) is called **projection**. It's called projection because we are \"projecting\" information from two-dimensions (e.g., women and their events as distinct dimensions) into one-dimension (e.g., women who attended the same event).\n",
    "\n",
    "We can weight the edges of the projection using different criteria. For instance, if two women attended several of the same events together, we can create an edge between them to carry a higher weight than say two other women who attended only one event together. In other words, we can weigh the edges of this projection based on the number of shared events women attended.\n",
    "\n",
    "Let's double check our list of women again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "women"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this in tow, let's use the `bipartite` graph and perform a weighted projection. In other words, we're going to take the information about women and the events the attended (a bitpartite network) into a network of women, where the link between them is the number of events they attended together. We'll call this projection `W`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = bipartite.weighted_projected_graph(D, women)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's list the nodes (i.e., the women). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(W.nodes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get the edgelist of women to see how often women co-attended events together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(W.edges(data=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that it's a unipartite network, we can run the first set of degree centrality measures. Let's try two: betweenness and closeness. \n",
    "\n",
    "Use both measures and apply it to `W`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bet_cen_W = nx.betweenness_centrality(W,weight='weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bet_cen_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clo_cen_W = nx.closeness_centrality(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clo_cen_W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do these centrality measures used on the unipartite **projection** network compare to the bipartite measures?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "# Checkpoint 6 of 8\n",
    "## Now you try!\n",
    "\n",
    "### We're going to take your bipartite network `B2` and project it to make unipartite network. Call it `B2_U`. \n",
    "\n",
    "### Calculate any two centrality scores you'd like from `B2_U`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## Community Detection\n",
    "\n",
    "\n",
    "![Neuron](Images/13_Network_Communities.png) \n",
    "\n",
    "Facebook data are a great place to start to study community detection analysis in social networks. Why? Think about your Facebook friends. They probably come from different aspects of your life: Some are your friends from college, others are your co-workers, and maybe some old friends from your hometown.\n",
    "\n",
    "Because your friends can be broken down into different groups like this, you may wonder if we could identify these different communities in your social network. Using community detection algorithms, we can break down a social network into different potentially overlapping communities. For instance, consider the image above of two hypothetical networks, where \"communities\" of nodes are distinctly color-coded. Notice that in many ways, these communities are both distinct and overlap. This could be the overlap of highschool and hometown friends; or college friends and graduate school friends; etc.\n",
    "\n",
    "There are a bunch of different strategies for finding communities in social network data. Like we've discussed with other methods in data science, the right way to detect communities depends on what intuition you have about your data. The algorithms that distinguish one community detection method from another, in particular, vary by what structural features they emphasize in the network. For instance, for the three we're going to talk about right now:\n",
    "\n",
    "- Modularity maximization tries to maximize the number of edges within a cluster, and minimize the number of edges going between clusters\n",
    "- The Girvan-Newman algorithm focuses on betweenness centrality\n",
    "- Clique percolation focuses on fully connected sets of nodes (i.e. cliques)\n",
    "\n",
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modularity maximization\n",
    "Let's start with the most popular of the three, modularity maximization. The main criterion for finding good communities here is that we want to maximize intra-community edges while minimizing inter-community edges. Formally, the algorithm tries to maximize the modularity of network, or the fraction of edges that fall within a community minus the expected fraction of edges if the edges were distributed by random. Good communities should have a high number of intra-community edges, so by maximizing the modularity, we detect dense communities that have a high fraction of intra-community edges.\n",
    "\n",
    "**Important notes:**\n",
    "1. Modularity maximization is popular because it is fast (relatively speaking). Community detection takes loads of computing power, and the simplicity of modularity maximization makes it faster to implement.\n",
    "2. **But** modularity maximization has serious flaws. It has trouble finding small communities (this has been called its [resolution limit](http://www.pnas.org/content/104/1/36)), and worse, modularity does not always have a global maximum, leading to an almost infinite number of \"right\" solutions according to modularity maximization (see [this paper](https://journals.aps.org/pre/abstract/10.1103/PhysRevE.81.046106) if you're curious). How badly this is going to affect your work depends on the size of your network and the types of communities you are expecting. Unless computing power is a limiting factor, it's usually better to avoid using this approach.\n",
    "\n",
    "Still, modularity maximization is probably the most popular community detection scheme out there, so you are better off knowing what it is. We're going to use the `community` module, provided for you as a folder in your Notebook directory. We're also going to read in `NetworkX` and `matplotlib` for when we plot these networks. We're also going to suppress any warning messages we get from plotting these networks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import community\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read in some Facebook data. These data are from SNAP (Stanford Network Analysis Project). They were collected data from a survey of Facebook users. (More information here: https://snap.stanford.edu/data/egonets-Facebook.html.) These data are very large, so many of things that we're going to run will probably take a bit longer than ususal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "G_fb = nx.read_edgelist(\"Data/facebook_combined.txt\", \n",
    "                        create_using = nx.Graph(), \n",
    "                        nodetype = int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get some basic information from these data using the function `info()`. \n",
    "\n",
    "This network is pretty large.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(nx.info(G_fb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's plot this network. Let's do it using a \"spring layout,\" like the one we learned about last week.\n",
    "\n",
    "**Note: This will take a few minutes to run!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's take a look at the network. Create network layout for visualizations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spring_pos = nx.spring_layout(G_fb)\n",
    "plt.axis(\"off\")\n",
    "nx.draw_networkx(G_fb, \n",
    "                 pos = spring_pos, \n",
    "                 with_labels = False, \n",
    "                 node_size = 35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run a community detection analysis from the package `community` (because NetworkX made the decision to not even support modularity maximization in its package) and cluster nodes together. First we're going to run the function called `.best_partition()` from `community` on `G_fb`, our `NetworkX` object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mod_communities = community.best_partition(G_fb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what's in the `mod_communities` object now. It looks like a really long dictionary. In fact, it's a dictionary where all of the keys are nodes in the network, `G_fb`. The values correspond to its community assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mod_communities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract the values (i.e., communities) from this `dictionary` and use a list and a set to find all of the unique values. This will give us a list of all of the communities that the algorithm detected. \n",
    "\n",
    "We see that it found a total of 16 communities, which it numbered from 0 to 15. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(set(list(mod_communities.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create a list for each node, indicating which \"community\" it happens to be in. Let's save this as `mod_values`. For every node in `G_fb`--designated by `G_fb.nodes()`--loop through each one and return its corresponding community allocation. \n",
    "\n",
    "Since `mod_communities` is a `dictionary`, we can use `mod_communities.get()` to return the community id (e.g., 0 through 15). We just have to pass in the node (as the key) to get the community ID (which returns the key's value). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mod_values = [mod_communities.get(node) for node in G_fb.nodes()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's draw the exact same network again, but let's color in the nodes based on which community they belong to using the list `values` that we just defined. \n",
    "\n",
    "We'll attach a lovely set of functions written by a [fellow on Stack Overflow](https://stackoverflow.com/questions/17511949/group-vertices-in-clusters-using-networkx?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa) that create a layout for your network according to a dictionary of nodes and communities. \n",
    "\n",
    "It's long...really long...so keep scrolling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def community_layout(g, partition):\n",
    "    \"\"\"\n",
    "    Compute the layout for a modular graph.\n",
    "\n",
    "\n",
    "    Arguments:\n",
    "    ----------\n",
    "    g -- networkx.Graph or networkx.DiGraph instance\n",
    "        graph to plot\n",
    "\n",
    "    partition -- dict mapping int node -> int community\n",
    "        graph partitions\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pos -- dict mapping int node -> (float x, float y)\n",
    "        node positions\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    pos_communities = _position_communities(g, partition, scale=3.)\n",
    "\n",
    "    pos_nodes = _position_nodes(g, partition, scale=1.)\n",
    "\n",
    "    # combine positions\n",
    "    pos = dict()\n",
    "    for node in g.nodes():\n",
    "        pos[node] = pos_communities[node] + pos_nodes[node]\n",
    "\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _position_communities(g, partition, **kwargs):\n",
    "\n",
    "    # create a weighted graph, in which each node corresponds to a community,\n",
    "    # and each edge weight to the number of edges between communities\n",
    "    between_community_edges = _find_between_community_edges(g, partition)\n",
    "\n",
    "    communities = set(partition.values())\n",
    "    hypergraph = nx.DiGraph()\n",
    "    hypergraph.add_nodes_from(communities)\n",
    "    for (ci, cj), edges in between_community_edges.items():\n",
    "        hypergraph.add_edge(ci, cj, weight=len(edges))\n",
    "\n",
    "    # find layout for communities\n",
    "    pos_communities = nx.spring_layout(hypergraph, **kwargs)\n",
    "\n",
    "    # set node positions to position of community\n",
    "    pos = dict()\n",
    "    for node, community in partition.items():\n",
    "        pos[node] = pos_communities[community]\n",
    "\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _find_between_community_edges(g, partition):\n",
    "\n",
    "    edges = dict()\n",
    "\n",
    "    for (ni, nj) in g.edges():\n",
    "        ci = partition[ni]\n",
    "        cj = partition[nj]\n",
    "\n",
    "        if ci != cj:\n",
    "            try:\n",
    "                edges[(ci, cj)] += [(ni, nj)]\n",
    "            except KeyError:\n",
    "                edges[(ci, cj)] = [(ni, nj)]\n",
    "\n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _position_nodes(g, partition, **kwargs):\n",
    "    \"\"\"\n",
    "    Positions nodes within communities.\n",
    "    \"\"\"\n",
    "\n",
    "    communities = dict()\n",
    "    for node, community in partition.items():\n",
    "        try:\n",
    "            communities[community] += [node]\n",
    "        except KeyError:\n",
    "            communities[community] = [node]\n",
    "\n",
    "    pos = dict()\n",
    "    for ci, nodes in communities.items():\n",
    "        subgraph = g.subgraph(nodes)\n",
    "        pos_subgraph = nx.spring_layout(subgraph, **kwargs)\n",
    "        pos.update(pos_subgraph)\n",
    "\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look and see how well the modularity maximization method did at finding these hidden groups.\n",
    "\n",
    "We're going to take our function `community_layout()`, passing in our network `G_fb` and the communities we detected `mod_communities`, and then plot we'll color in (and position) the nodes from the graph based on what community it belongs to. \n",
    "\n",
    "You'll see a nice seperation! \n",
    "\n",
    "**Note: This will take a few minutes to run!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.axis(\"off\")\n",
    "nx.draw_networkx(G_fb, \n",
    "                 pos = community_layout(G_fb, mod_communities), #This is our function community_layout()\n",
    "                 cmap = plt.get_cmap(\"jet\"), \n",
    "                 node_color = mod_values, # Let's color the node based on which community it happens to be in. \n",
    "                 node_size = 35, \n",
    "                 with_labels = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "# Checkpoint 7 of 8\n",
    "## Now you try!\n",
    "\n",
    "### You the Star Wars graph from earlier `SW_G` and let's see what communities we can uncover. \n",
    "\n",
    "### Use the function provided here `community_layout()` and pass in the Stars Wars graph and the partitions from `community.best_partition()` to plot the Star Wars graph based on each node's community assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "### Clique percolation\n",
    "Finally we'll try clique percolation, which as you might guess works by finding cliques in the network. In practice, it tries to connect k-cliques across the network to form communities. \n",
    "\n",
    "Say you have a clique with three members. Clique percolation looks for any other 3-cliques that it might be connected to, and any 3-cliques that those might be connected to, and so on. All those nodes taken together form communities.\n",
    "\n",
    "Clique percolation is much faster than Girvan-Newman, and tends to be better at finding community structure than modularity maximization. If you are interested in the structure of a network where flows matter, Girvan-Newman is probably best. If you are more interested in the density of connections, clique percolation is probably best.\n",
    "\n",
    "Again, this comes implemented in the NetworkX package as `k_clique_communites`. First we'll try to use clique percolation with our Hartford network, but we'll run it for 2-cliques, 3-cliques, AND 4-cliques, separately to see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First let's get the community detection functions from community in networkx\n",
    "from networkx.algorithms import community as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cp_communities2 = cm.k_clique_communities(hartford_ud, 2) # 2-cliques\n",
    "cp_communities3 = cm.k_clique_communities(hartford_ud, 3) # 3-cliques\n",
    "cp_communities4 = cm.k_clique_communities(hartford_ud, 4) # 4-cliques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function spits out an iterator similar to the one the Girvan-Newman function gave. We'll process them in the similar way. We'll have to add a statement to our loops just in case a node is not part of any of our k-cliques.\n",
    "\n",
    "Let's turn our code into a function, since we're running it over and over. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_membership(graph, community_obj, k):\n",
    "    member_list = list(community_obj)\n",
    "    member_dict = {}\n",
    "    print('There are', len(member_list), 'communities according to the clique percolation algorithm for', k, '-cliques \\n')\n",
    "    for node in graph.nodes():\n",
    "        for community in range(0, len(member_list)): # Check every community for the node\n",
    "            if node in member_list[community]:\n",
    "                member_dict[node] = community\n",
    "                break\n",
    "            elif community == (len(member_list)-1) and node not in member_list[community]:\n",
    "                member_dict[node] = 999   \n",
    "    return(member_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the function and get the membership dictionaries.\n",
    "\n",
    "The function should output the number of communities according to each approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cp_membership2 = get_membership(hartford_ud, cp_communities2, 2)\n",
    "cp_membership3 = get_membership(hartford_ud, cp_communities3, 3)\n",
    "cp_membership4 = get_membership(hartford_ud, cp_communities4, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the 2-Clique model. \n",
    "\n",
    "First, get the list of group memberships to color our graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cp_values2 = [value for value in cp_membership2.values()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 2-Cliques\n",
    "plt.axis(\"off\")\n",
    "nx.draw_networkx(hartford_ud,\n",
    "                 pos = community_layout(hartford_ud, cp_membership2),\n",
    "                 cmap = plt.get_cmap(\"jet\"), \n",
    "                 node_color = cp_values2, \n",
    "                 node_size = 35, \n",
    "                 with_labels = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 3 and 4 cliques (`cp_values3` and `cp_values4`), we need to do a few extra steps. \n",
    "\n",
    "Namely, we have to subset the graph to get rid of the unconnected nodes that we coded as 999. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3-Cliques\n",
    "# First we have to subset the graph to get rid of the unconnected nodes that we coded as 999\n",
    "node_subset3 = [k for k in cp_membership3.keys() if cp_membership3[k] != 999]\n",
    "cp_values3 = [cp_membership3[k] for k in cp_membership3.keys() if cp_membership3[k] != 999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.axis(\"off\")\n",
    "nx.draw_networkx(hartford_ud.subgraph(node_subset3),\n",
    "                 pos = community_layout(hartford_ud, cp_membership3),\n",
    "                 cmap = plt.get_cmap(\"jet\"), \n",
    "                 node_color = cp_values3, \n",
    "                 node_size = 35, \n",
    "                 with_labels = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 4-Cliques\n",
    "# First we have to subset the graph to get rid of the unconnected nodes that we coded as 999\n",
    "node_subset4 = [k for k in cp_membership4.keys() if cp_membership4[k] != 999]\n",
    "cp_values4 = [cp_membership4[k] for k in cp_membership4.keys() if cp_membership4[k] != 999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.axis(\"off\")\n",
    "nx.draw_networkx(hartford_ud.subgraph(node_subset4),\n",
    "                 pos = community_layout(hartford_ud, cp_membership4),\n",
    "                 cmap = plt.get_cmap(\"jet\"), \n",
    "                 node_color = cp_values4, \n",
    "                 node_size = 35, \n",
    "                 with_labels = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clique percolation with 4-cliques is essentially useless for the Hartford network because there is only one 4-clique in the entire graph! \n",
    "\n",
    "That said, in other graphs 4-cliques might be the **most** useful choice. Using 3-cliques finds a whole lot of structure, but it is pretty chaotic, while 2-cliques produces a graph that is very similar to `Girvan-Newman`.\n",
    "\n",
    "We can also compare this with the modularity maximization algorithm. Let's run that quickly on the Hartford data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mod_communities = community.best_partition(hartford_ud)\n",
    "mod_values = [mod_communities.get(node) for node in hartford_ud.nodes()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.axis(\"off\")\n",
    "nx.draw_networkx(hartford_ud,\n",
    "                 pos = community_layout(hartford_ud, mod_communities),\n",
    "                 cmap = plt.get_cmap(\"jet\"), \n",
    "                 node_color = mod_values, \n",
    "                 node_size = 35, \n",
    "                 with_labels = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is much more aggressive than our other algorithms in partioning this network. The results look fairly similar to those produced the Girvan-Newman algorithm.\n",
    "\n",
    "How you deploy community detection should depend on the nuances of your data. Don't choose an algorithm blindly.\n",
    "\n",
    "Quick (and overly simplified) summary: \n",
    "\n",
    "1. **Modularity maximization** is fast but inaccurate. It maximizes edges within a community and minimizes those going between communities.\n",
    "2. The **Girvan-Newman algorithm** is slow but accurate. It focuses on betweenness, making it ideal for finding communities based on the *flow* of resources, diseases, information, etc. in a graph.\n",
    "3. **Clique percolation** is faster than Girvan-Newman (but slower than modularity maximization in most cases). Building itself up from connected cliques, its accuracy depends on your choice of k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "# Checkpoint 8 of 8\n",
    "## Now you try!\n",
    "\n",
    "### Run either a 2-Clique, 3-Clique, or a 4-Clique algorithm on your Star Wars graph `SW_G`. \n",
    "### Be sure to first remove any `999` values for nodes not assigned to any clique. \n",
    "\n",
    "### Finally, plot the graph based on the community assignments from your n-clique choice. How does it compare to the earlier plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
