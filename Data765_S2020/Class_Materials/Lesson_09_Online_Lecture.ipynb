{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font face=\"times\"><font size=\"6pt\"><p style = 'text-align: center;'> The City University of New York, Queens College\n",
    "\n",
    "<font face=\"times\"><font size=\"6pt\"><p style = 'text-align: center;'><b>Introduction to Computational Social Science</b><br/><br/>\n",
    "\n",
    "<p style = 'text-align: center;'><font face=\"times\"><b>Lesson 09 | Natural Language Processing I: Text Statistics</b><br/><br/>\n",
    "\n",
    "<p style = 'text-align: center;'><font face=\"times\"><b>10 Checkpoints</b><br/><br/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# Begin Lesson 09\n",
    "# Text as Data-An Introduction to Natural Language Processing (NLP)\n",
    "\n",
    "NLP stands for Natual Language Processing and it involves a huge variety of tasks such as:\n",
    "- Automatic summarization.\n",
    "- Discourse analysis.\n",
    "- Machine translation.\n",
    "- Named entity recognition.\n",
    "- Part-of-speech tagging.\n",
    "- Parsing.\n",
    "- Question answering.\n",
    "- Relationship extraction.\n",
    "- Sentiment analysis.\n",
    "- Speech recognition.\n",
    "- Topic segmentation.\n",
    "- Word segmentation.\n",
    "- Information retrieval.\n",
    "- Information extraction.\n",
    "- Speech processing.\n",
    "\n",
    "The key idea is that there are times when we would much prefer to process a heap of text without having to read it ourselves. Reading is, of course, energy- and time-intensive. Reading the entire corpus of fiction written in English from the 15th century to the 18th century, for instance, would take years. But with NLP we can have a computer do the same thing in a relatively short time, letting us answer fascinating questions that would otherwise be out of reach.\n",
    "\n",
    "Python is built in with a very mature regular expression library, which is the building block of natural language processing. Traditionally, in the Python ecosystem the Natural Language Processing Toolkit, abbreviated as `NLTK`, has been until recently the only working choice.\n",
    "\n",
    "Natural Language Processing (NLP) is often taught at the academic level from the perspective of computational linguists. However, as social scientists, we have a richer view of the natural language world - unstructured data that by its very nature has latent information that is important to humans. NLP practioners have benefited from machine learning techniques to unlock meaning from large corpora, and we’ll explore how to do that particularly with Python and with the Natural Language Toolkit (NLTK). \n",
    "\n",
    "NLTK is an excellent library for machine-learning based NLP, written in Python by experts from both academia and industry. Python allows you to create rich data applications rapidly, iterating on hypotheses. The combination of Python + NLTK means that you can easily add language-aware data products to your larger analytical workflows and applications. \n",
    "    \n",
    "**Requirements: You will need to have NLTK installed, along with downloading the corpus for stopwords. To download everything with a conda installation, see below.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "# Introduction to NLP with NLTK\n",
    "\n",
    "## Quick Overview of NLTK\n",
    "`NLTK` stands for the Natural Language Toolkit and it provides a combination of natural language corpora, lexical resources, and example grammars with language processing algorithms, methodologies and demonstrations for a very pythonic \"batteries included\" view of Natural Language Processing.   \n",
    "\n",
    "As such, `NLTK` is perfect for researh driven (hypothesis driven) computational social science projects. Its suite of libraries includes:\n",
    "\n",
    "- tokenization, stemming, and tagging\n",
    "- chunking and parsing\n",
    "- language modeling\n",
    "- classification and clustering\n",
    "- logical semantics\n",
    "\n",
    "`NLTK` is also a useful pedagogical resource for learning NLP with Python and serves as a starting place for producing production grade code that requires natural language analysis. `NLTK` provides a variety of tools that can be used to explore the linguistic domain but is not a lightweight dependency that can be easily included in other workflows, especially those that require unit and integration testing or other build processes. This stems from the fact that `NLTK` includes a lot of added code but also a rich and complete library of corpora that power the built-in algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Installing NLTK\n",
    "\n",
    "This notebook has a few dependencies, most of which can be installed via the python package manger - `pip` or using `conda`. Here, we're going to use `conda`. \n",
    "\n",
    "1. NLTK\n",
    "2. The NLTK corpora \n",
    "3. The BeautifulSoup library\n",
    "4. The gensim libary\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're installing this with PythonAnywhere (or anywhere else, for that matter, like for your future jobs) then use the following in Python:\n",
    "\n",
    "**Note:** These requirements may already be installed, in which case you should receive some sort of output like `\"Requirement already satisifed:\"`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip3.6 install --user gensim\n",
    "!pip3.6 install --user nltk\n",
    "!pip3.6 install --user beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's import some of the packages we just installed (e.g., nltk, gensim) and some others we've used (e.g., pandas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import nltk\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've imported these modules, we need download the necessary resources for `NLTK`. \n",
    "\n",
    "Uncomment the commands below and run.\n",
    "\n",
    "**Note:** This will likely take some time, especially if you're setting this up on your local machine or personal computer, depending on your internet speeds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('tagsets')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('sinica_treebank')\n",
    "nltk.download('reuters')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## Preprocessing Text Data - Tokenization, Stemming, Lemmatization, and Stop Words\n",
    "\n",
    "In English (and many other languages) - context indicate gender, tense, quantity, etc. but these subleties might not be necessary. It is helpful for many applications to normalize these word forms (especially applications like search) into some canonical word for further exploration. This is where stemming and lemmatization come in. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fundamental \"datum\" for NLP is the token. Tokens are another way of saying terms and words that are made distinct by punctuation, capitalization, or verb conjugations (more on this in a bit). It is a necessary preprocessing step for NLP applications. Tokenization involves segmenting text into tokens. Let's look at the following string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"Are you crazy? I don't know.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It often makes sense to force the text into lowercase. Recall from our first week that computers view \"Crazy\" as a different string from \"crazy\" and different still from \"CrazY\" due to the placement of the upper-case letters. If you want to count these as the same term, turning each of these strings into lowercase helps solve that problem.\n",
    "\n",
    "Let's give that a try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the text looks like now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on, be aware that there are in fact scenarios where you might care about the difference between upper-case and lower-case spellings, and thus would **not** want to reduce text to lower-case in NLP work. In English, for instance, \"China\" has a different meaning than \"china\". Further, some languages like German use capitalization to indicate the difference between nouns and non-nouns that are spelled in the same manner. If you expect distinctions like these to matter in your corpora, it might be wise to leave the casing as-is, since you risk conflating the terms by reducing the text to lower-case.\n",
    "\n",
    "Back to NLP! To work with our text, we need to split the sentence up into its tokens. A simple method is just to split based on white space. \n",
    "\n",
    "***Note*** that this doesn't work for many other languages (e.g., Japanese, Greek, Chinese). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now explore tokenization as provided by two NLP tools: `NLTK` and `gensim`. First, look at how `NLTK` tokenizes words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `nltk` package is great to use. Now, let's look at how `Gensim` is handling tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(gensim.utils.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which one do you think worked better? As we'll see, we'll want to remove punctuation and filler words that offer little added meaning to our analyses (e.g., prepositions, articles, pronouns, etc.). NLTK happens to keep more of this information. Gensim, on the other hand, may have trouble disaggregating \"don't\" from \"don\" (e.g. Don Draper) after tokenization, depending on how we process the data (there are simple ways to deal with this).\n",
    "\n",
    "One of the key takeaways on working text as data is that no one approach will be perfect. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "# Checkpoint 1 of 10\n",
    "## Now you try!\n",
    "\n",
    "### Make-up your own sentence and save it as `my_sentence`. \n",
    "\n",
    "### Be sure to include punctuation, random capitalization, etc. This isn't an English class, so no need to worry about proper grammar. \n",
    "\n",
    "### Change the text to lower case, convert the text to a list of strings based on the white spaces, and use `gensim` to tokenize it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "### Sidenote: Non-English and Non-Romanized Langauges\n",
    "\n",
    "Working with text that is not in English and/or does not use the roman alphabet is common. These techniques are just as applicable, however our focus here will be English. Just to give an example though: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = \"今天天氣如何?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import sinica_treebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sinica_treebank.tagged_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "### Stemming\n",
    "\n",
    "[Stemming](http://en.wikipedia.org/wiki/Stemming) chops off affixes to get the root stem of the word. It is the process of reducing a word to its base/stem/root form. Most stemmers are pretty basic and just chop off standard affixes indicating things like tense (e.g., \"-ed\") and possessive forms (e.g., \"-'s\"). For instance:\n",
    "\n",
    "    running --> run\n",
    "    flowers --> flower\n",
    "    geese   --> geese \n",
    "\n",
    "Once our tokens are stemmed, we can rest easy knowing that *BuzzFeed* and *BuzzFeed's* are now being counted together as... *buzzfe*? Don't worry: although this may look weird, it's pretty standard behavior for stemmers and won't affect our analysis (much). We also (probably) won't show the stemmed words to users -- we'll normally just use them for internal analysis or indexing purposes.\n",
    "\n",
    "There are several stemmers available:\n",
    "\n",
    "    - Lancaster (English, newer and aggressive)\n",
    "    - Porter (English, original stemmer)\n",
    "    - Snowball (Many langauges, newest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`NLTK` has several stemmers already for our use (listed above). Let's import them first and we can use them to stem a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's stem the following sentence: \n",
    "\n",
    "\"The women running in the fog passed bunnies working as computer scientists.\"\n",
    "\n",
    "Let's use the `word_tokenize` function form `NLTK` and then save these tokenized words in a `list` structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = list(nltk.word_tokenize(\"The women running in the fog passed bunnies working as computer scientists.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's initialize the three different stemmers that we have at our disposal. \n",
    "\n",
    "Each has their strengths and weaknesses, so let's test them all and see how they each work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "snowball = SnowballStemmer('english')\n",
    "lancaster = LancasterStemmer()\n",
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have a for loop that will use these three stemmers - snowball, lancaster, and porter - to stem our above list. It will print out how our previous sentence is stemmed using these three different approaches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for stemmer in (snowball, lancaster, porter):\n",
    "    stemmed_text = [stemmer.stem(t) for t in text]\n",
    "    print(\" \".join(stemmed_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `snowball` stemmer produced the sentence as: \"the women run in the fog pass bunni work as comput scientist .\"\n",
    "\n",
    "The `lancaster` stemmer produced: \"the wom run in the fog pass bunny work as comput sci .\"\n",
    "\n",
    "The `porter` stemmer produced: \"the women run in the fog pass bunni work as comput scientist .\"\n",
    "\n",
    "As you can surmise, no one stemmer is going to be perfect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "# Checkpoint 2 of 10\n",
    "## Now you try!\n",
    "\n",
    "### Come up with another sentence, different than the one you used in the first checkpoint. \n",
    "\n",
    "### Follow the same procedures here and stem the sentence using all three techniques: `snowball`, `lancaster`, and `porter`. \n",
    "### Print out the stemmed sentences. What one did the best? How do they differ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "### Lemmatization\n",
    "\n",
    "Although the stemmer very helpfully chopped off pesky affixes (and made everything lowercase to boot), there are some word forms that give stemmers indigestion, especially *irregular* words. While the process of stemming typically involves rule-based methods of stripping affixes (making them small & fast), **lemmatization** involves dictionary-based methods to derive the canonical forms (i.e., *lemmas*) of words. For example, *run*, *runs*, *ran*, and *running* all correspond to the lemma *run*. \n",
    "\n",
    "    women   --> woman\n",
    "    foxes   --> fox\n",
    "    sheep   --> sheep\n",
    "    \n",
    "However, lemmatizers are generally big, slow, and brittle due to the nature of the dictionary-based methods, so you'll only want to use them when necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example below compares the output of the Snowball stemmer with the WordNet lemmatizer (also distributed with NLTK). Notice that the lemmatizer correctly converts *women* into *woman*, while the stemmer turns *lying* into *lie*. Additionally, both replace *eyes* with *eye*, but neither of them properly transforms *told* into *tell*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp_sent = \"Several people told me I have an amazing voice.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's establish the lemmatizer with one in the `NLTK` toolkit called `WordNetLemmatizer()`. First import it, and then set it up as `lemmatizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemmatizer = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print([lemmatizer.lemmatize(t) for t in nltk.word_tokenize(temp_sent)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare this to one of the stemmers, let's say the `Snowball` stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print([snowball.stem(t) for t in nltk.word_tokenize(temp_sent)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the stemmer does a much better job than the lemmatizer. \n",
    "\n",
    "Again, there's no one silver bullet that will always work perfectly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "# Checkpoint 3 of 10\n",
    "## Now you try!\n",
    "\n",
    "### Use the sentence you came up with for the second checkpoint and use the lemmatizer. How did the lemmatize method do compared to the stemming?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "### Stop Words and Filtering Out Punctuation\n",
    "\n",
    "Many extremely common words which would appear to be of little value are excluded from the vocabulary entirely. These words include conjunctions (e.g., and, but, or, etc.) prepositions (e.g., between, under, around, etc.), articles (e.g., the, an, etc.), pronouns (e.g., him, her, they, etc.), etc. As you may have noticed in the above examples, we included them in our analyses. \n",
    "\n",
    "They are called [*stop words*](http://en.wikipedia.org/wiki/Stop_words). What we need to do is filter them out in order to focus on just the important material.\n",
    "\n",
    "While there is no single, definitive list of stop words, but `NLTK` provides a decent start. Let's load it up and take a look at what we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted(nltk.corpus.stopwords.words('english'))[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the stemming code from above, but now let's filter out these stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print([snowball.stem(t) for t in nltk.word_tokenize(temp_sent) if t not in nltk.corpus.stopwords.words('english')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we compare it to the original, we can see that we've removed some stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print([snowball.stem(t) for t in nltk.word_tokenize(temp_sent)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "# Checkpoint 4 of 10\n",
    "\n",
    "## Now you try!\n",
    "\n",
    "###  Use the sentence you used in the second and third checkpoints and use list comprehension (e.g., `[x for x in some_list]`) to remove stop words from your sentence. \n",
    "\n",
    "### DO NOT LEMMATIZE OR STEM HERE, AS SHOWN IN THE ABOVE EXAMPLE! Just remove the stopwords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Punctuation Removal \n",
    "\n",
    "Finally, there's one last issue: punctuation. As you can see from above, a period is being considered a token. You might have also noticed that apostrophes were also being included after tokenization, stemming, and/or lemminitization. \n",
    "\n",
    "To address this, let's use the `string` package to remove them. First, import the module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get the punctuations we want to remove and save it as `punctuation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "punctuation = string.punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see what punctuation is included and will subsequently be removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is, in fact, one large string of all of the different punctuations we would want to strip away from any token and sentence. The great thing about `Python` is that we can easily test if a token is in this string.\n",
    "\n",
    "Let's add to our stemmer/stopword code above, and add in a second if statement to stop tokens that are in the punctuation list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print([snowball.stem(t) for t in nltk.word_tokenize(temp_sent) if t not in punctuation and t not in nltk.corpus.stopwords.words('english')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the above output to what we were getting earlier, we now removed the period. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "# Checkpoint 5 of 10\n",
    "\n",
    "## Now you try!\n",
    "\n",
    "###  Use the sentence you used in the second, third, and fourth checkpoints and use list comprehension (e.g., `[x for x in some_list]`) to remove punctuation from your sentence. \n",
    "\n",
    "### DO NOT LEMMATIZE, STEM, or REMOVE STOP WORDS HERE, AS SHOWN IN THE ABOVE EXAMPLE! Just remove punctuation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "# Bringing It All Together\n",
    "\n",
    "Typical normalization of text for use as features in machine learning models looks something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemmatizer  = WordNetLemmatizer()\n",
    "stopwords   = set(nltk.corpus.stopwords.words('english'))\n",
    "punctuation = string.punctuation #This is from the module string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, I defined a function called `normalize` that takes in text as one long string. It first tokenizes and lemmatizes it; and then it removes stopwords and punctuation. \n",
    "\n",
    "It uses `nltk` to go break up the sentence into tokens and uses a for loop to process each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    for token in nltk.word_tokenize(text):\n",
    "        token = token.lower()\n",
    "        token = lemmatizer.lemmatize(token)\n",
    "        if token not in stopwords and token not in punctuation:\n",
    "            yield token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it out with a simple sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(list(normalize(\"The eagle flies at midnight.\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint 6 of 10\n",
    "\n",
    "## Now you try!\n",
    "\n",
    "### Write a new sentence and save it as a string. \n",
    "\n",
    "### Use the `normalize()` function I wrote and use it to tokenize, lemmatize, and remove stopwords and punctuation. \n",
    "### Print out the output, but be sure to capture the output as a `list()`, as the example shows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## Frequency Counts\n",
    "\n",
    "Here's a little secret: much of NLP (and data science, for that matter) boils down to counting things. \n",
    "\n",
    "If you've got a bunch of data that needs *analyzin'* but you don't know where to start, counting things is usually a good place to begin. Sure, you'll need to figure out exactly what you want to count, how to count it, and what to do with the counts, but if you're lost and don't know what to do, **just start counting**.\n",
    "\n",
    "`NLTK` provides two very excellent classes to enable these frequency analyses:\n",
    "\n",
    "- `FreqDist`\n",
    "- `ConditionalFreqDist` \n",
    "\n",
    "And these two classes serve as the foundation for most of the analyses that we will conduct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps we'd like to begin (as is often the case in NLP) by examining the words that appear in our document. To do that, we'll first need to tokenize the text string into discrete words. Since we're working with English, this isn't so bad, but if we were working with a non-whitespace-delimited language like Chinese, Japanese, or Korean, it would be much more difficult, as discussed above.\n",
    "\n",
    "In the code snippet below, we're using two of NLTK's tokenize methods to first chop up the article text into sentences, and then each sentence into individual words. (Technically, we didn't need to use <code>sent_tokenize()</code>, but if we only used <code>word_tokenize()</code> alone, we'd see a bunch of extraneous sentence-final punctuation in our output.) \n",
    "\n",
    "By printing each token alphabetically, along with a count of the number of times it appeared in the text, we can see the results of the tokenization. Notice that the output contains some punctuation & numbers, hasn't been loweredcased, and counts *BuzzFeed* and *BuzzFeed's* separately. We'll tackle some of those issues next.\n",
    "\n",
    "Let's begin with a longer string, the opening of a \"Tale of Two Cities\" by Charles Dickens:\n",
    "\n",
    "`It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way—in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only.`\n",
    "\n",
    "- `sent_tokenize`: This tokenizer divides a text (e.g., text from a document or a paragraph) into a list made up of sentences (i.e., sent_ == 'sentence'). \n",
    "    \n",
    "       \n",
    "- `word_tokenize`: This tokenizer uses regular expressions to tokenize text as in Penn Treebank. It assumes that the text has already been segmented into sentences, e.g. using ``sent_tokenize()``.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = 'It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way—in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = [word for sentence in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sentence.lower()) if word not in stopwords and word not in punctuation]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what this looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have tokenized this long string, let's get a frequency count of how often each token occurs. \n",
    "\n",
    "This is going to take a few steps. First we need the unique list of tokens. We can do this with the code \n",
    "\n",
    "`set(tokens)`\n",
    "\n",
    "Let's look at these tokens alphabetically, say the first 30. We don't need to look at them alphabetically; there are lots of ways to look at which terms happen more frequently. Here, I choose to look at it this way as just one helpful way to explore a corpus. \n",
    "\n",
    "We can do so using `sorted()`, which will sort this list of tokens.\n",
    "\n",
    "We can pass in each individual token from this unique list back into `tokens` to get its count. In other words, let's create a for loop that will go through each of these unique tokens and return how often it occurs in this text.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for individual_token in sorted(set(tokens)):\n",
    "    print(individual_token + ' | Count: ' + str(tokens.count(individual_token)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus far, we've been working with lists of tokens that we're manually sorting, uniquifying, and counting -- all of which can get to be a bit cumbersome. Fortunately, NLTK provides a data structure called <code>FreqDist</code> that makes it more convenient to work with these kinds of frequency distributions. The code snippet below builds a <code>FreqDist</code> from our list of stemmed tokens, and then displays the top 25 tokens appearing most frequently in the text of our article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fdist  = nltk.FreqDist(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `fdist` is a special `dictionary` with value-key pairing. Let's use a for loop and see what's inside. \n",
    "\n",
    "To do so, let's use the `items()` method to extract the value-key pairs and turn it into a list. (This makes dealing with it much easier.) Now, let's loop through each item and print out both the term and the frequency count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for term, count in list(fdist.items()):\n",
    "    print(term + \" \" + str(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this good. However, this text is short. If it were longer, we may not be able to discern what terms are more frequent. Thankfully, `fdist` has a method called `.most_common()` that will tell us the \"n\" most common terms found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(fdist.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "# Checkpoint 7 of 10\n",
    "## Now you try!\n",
    "\n",
    "### Copy and paste a paragraph from a news article (e.g., CNN, BBC, etc.). Save this paragraph as a string called `my_news_article`. \n",
    "\n",
    "### Remove stop words and punctuation and tokenzie the terms in the sentence and calculate the frequency distribution. \n",
    "\n",
    "### What are the 10 most common terms in your paragraph?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Scaling Up\n",
    "\n",
    "Let's test this out with a much bigger corpus. Included with `nltk` is a corpus of news articles from Reuters, an international news agency. Let's define this corpus to a variable called `reuters`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reuters = nltk.corpus.reuters # Corpus of news articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's convert the words in this corpus into counts, just as we did before. \n",
    "\n",
    "***Note:*** This may take a few moments to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counts  = nltk.FreqDist(reuters.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With counts, we can get the following descriptive features of the `reuters` corpus: \n",
    "\n",
    "- The vocabulary (unique words)\n",
    "- The count of words\n",
    "- The lexical diversity (the ratio of word count to vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab   = len(counts.keys()) #Number of unique terms\n",
    "words   = sum(counts.values()) #Total count of the words\n",
    "lexdiv  = float(words) / float(vocab) #Lexcial diversity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see what the `reuters` corpus looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Corpus has %i types and %i tokens for a lexical diversity of %0.3f\" % (vocab, words, lexdiv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as before, we can calculate the n most common tokens in the corpus. Let's look at the n=40 most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(counts.most_common(40))  # The n most common tokens in the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also find the most frequent token using `.max()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(counts.max()) # The most frequent token in the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also calculate what percentage of the entire corpus a particular term is. For instance, let's consider \"year\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts.freq('year') * 100 # percentage of the corpus for this token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word \"year\" is only 0.367% of the total corpus. In other words, it's fairly small.\n",
    "\n",
    "Let's test this out, for the 20 most common tokens (see above), let's see what the percentage they make up in the corpus. \n",
    "\n",
    "(Here, I'm going to use the function `round()` to round it off to two significant digits.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for most_common, count_ in counts.most_common(20):\n",
    "    print(\"Token \" + most_common + \" is \" + str(round(counts.freq(most_common) * 100,2)) + \"% of the corpus.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot these counts as well with the built-in method `.plot()`.\n",
    "\n",
    "**Note:** You'll also need to use `%matplotlib inline` in order to show the plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts.plot(200, cumulative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "# Checkpoint 8 of 10\n",
    "## Now you try!\n",
    "\n",
    "### Read in another sample corpus from `nltk`. Let's use the corpus of U.S. president \"State of the Union\" speeches. \n",
    "\n",
    "### First, download this corpus:\n",
    "\n",
    "```Python \n",
    "    nltk.download('state_union')\n",
    "```\n",
    "\n",
    "### Then, import the corpus:\n",
    "\n",
    "```Python\n",
    "    from nltk.corpus import state_union\n",
    "```\n",
    "\n",
    "### Calculate its lexical diversity. Then, calculate what percentage of how frequently the most popular term occurs in this corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## N-Grams\n",
    "\n",
    "Most of the work we've seen so far only considers single terms. However, terms have modifiers like adjectives and adverbs (e.g., 'American history' or 'wonderfully dull'). Seperating terms individually ignores the specific order that terms appear in, which potentially loses their meaning. \n",
    "\n",
    "To that end, we often use n-grams. They are simply all combinations of adjacent terms that you can find in your source text. Here, N is the number of terms that are adjacent and combined together to form one term. As an example, \"hello, world!\" contains the following:\n",
    " - Two unigrams (N=1): (1) Hello (2) World \n",
    " - One bigram (N=2): (1) Hello World \n",
    " \n",
    "Or, consider the sentence \"My name is Charlie Gomez\": it consists of \n",
    " - Five uni-grams (N=1):(1) My, (2) Name, (3) Is, (4) Charile, (5) Gomez \n",
    " - Four bi-grams (N=2): (1) My Name, (2) Name Is, (3) Is Charlie, (4) Charlie Gomez\n",
    " - Three tri-grams (N=3): (1) My Name Is, (2) Name Is Charlie, (3) Is Charlie Gomez \n",
    " - Two quad-grams (N=4): My Name Is Charlie, (2) Name Is Charlie Gomez\n",
    " - One quint-gram (N=5): My Name Is Charlie Gomez\n",
    "\n",
    "The basic point of n-grams is that they capture the language structure from a statistical point of view, like what term is likely to follow the given one. The longer the n-gram (the higher the N), the more context you have to work with. Optimum length really depends on the application – if your n-grams are too short, you may fail to capture important differences. On the other hand, if they are too long, you may fail to capture the “general knowledge” and only stick to particular cases.\n",
    "\n",
    "Thankfully, `NLTK` takes care of this for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take our test from before and find all of the bi-grams (N=2). This is accomplished with the `ngrams()` function. \n",
    "\n",
    "We need to split the text into a list of terms (i.e., `text.split()`) and pass in N that we want; here we want N=2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bi_freq = nltk.FreqDist(ngrams(text.split(),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take a look. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bi_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of this will be noise, however, looking at the most common bigrams is helpful. Let's loop through `bi_freq` and see what the 10 most common bigrams are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for (w1, w2), count in bi_freq.most_common(10):\n",
    "    print(w1, w2, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat every thing we just did for bigrams (N=2) and extend it to trigrams (N=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tri_freq = nltk.FreqDist(ngrams(text.split(), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for (w1, w2, w3), count in tri_freq.most_common(10):\n",
    "    print(w1, w2, w3, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extend this out all the way to n=5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for ngram in nltk.ngrams([\"The\", \"bear\", \"walked\", \"in\", \"the\", \"woods\", \"at\", \"midnight\"], 5):\n",
    "    print(ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "# Checkpoint 9 of 10\n",
    "## Now you try! \n",
    "\n",
    "### Using the \"State of the Union\" speech corpus from the previous checkpoint. Pick **ONE** speech and calculate the frequency of all bigrams and trigrams in it.  \n",
    "### What stands out? Do these words make sense?\n",
    "\n",
    "### *NOTE*: The speeches are in chronological order, so you can think about what president said what and when. Can you guess which president is speaking based on the bi-grams and tri-grams?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "*** \n",
    "\n",
    "## Term Frequency - Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "Consider a document that contains the word *baseball* 8 times. You might think, \"wow, *baseball* isn't a stop word, and it appeared rather frequently here, so it's probably important.\" And you might be right. But what if that document is actually an article posted on a baseball blog? Won't the word *baseball* appear frequently in nearly every post on that blog? In this particular case, if you were generating a summary of this document, would the word *baseball* be a good indicator of importance, or would you maybe look for other words that help distinguish or differentiate this blog post from the rest?\n",
    "\n",
    "Context is essential. What really matters here isn't the raw frequency of the number of times each word appeared in a document, but rather the **relative frequency** comparing the number of times a word appeared in this document against the number of times it appeared across the rest of the collection of documents. \"Important\" words will be the ones that are generally rare across the collection, but which appear with an unusually high frequency in a given document.\n",
    "\n",
    "We'll calculate this relative frequency using a statistical metric called [term frequency - inverse document frequency (TF-IDF)](http://en.wikipedia.org/wiki/Tf%E2%80%93idf). We could implement TF-IDF ourselves using NLTK, but rather than bore you with the math, we'll take a shortcut and use the TF-IDF implementation provided by the [scikit-learn](http://scikit-learn.org/) machine learning library for Python.\n",
    "\n",
    "### So what is TF-IDF?\n",
    "\n",
    "TF-IDF stands for *term frequency-inverse document frequency*, and the tf-idf weight is a weight often used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus. Variations of the tf-idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query.\n",
    "\n",
    "One of the simplest ranking functions is computed by summing the tf-idf for each query term; many more sophisticated ranking functions are variants of this simple model.\n",
    "\n",
    "Typically, the tf-idf weight is composed by two terms: the first computes the normalized Term Frequency (TF), aka. the number of times a word appears in a document, divided by the total number of words in that document; the second term is the Inverse Document Frequency (IDF), computed as the logarithm of the number of the documents in the corpus divided by the number of documents where the specific term appears.\n",
    "\n",
    "**TF: Term Frequency**, which measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization: \n",
    "\n",
    "    TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).\n",
    "\n",
    "**IDF: Inverse Document Frequency**, which measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following: \n",
    "\n",
    "    IDF(t) = log_e(Total number of documents / Number of documents with term t in it).\n",
    "\n",
    "See below for a simple example.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Consider a document containing 100 words where the word \"cat\" appears 3 times. \n",
    "\n",
    "The term frequency (i.e., tf) for cat is then (3 / 100) = 0.03. Now, assume we have 10 million documents in our corpus, and the word cat appears in one thousand of these. Then, the inverse document frequency (i.e., idf) is calculated as log(10,000,000 / 1,000) = 4. Thus, the Tf-idf weight is the product of these quantities: 0.03 * 4 = 0.12."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a Term-Document Matrix\n",
    "\n",
    "We'll use scikit-learn's <code>TfidfVectorizer</code> class to construct a [term-document matrix](http://en.wikipedia.org/wiki/Document-term_matrix) containing the TF-IDF score for each word in each document in the Reuters Corpus. In essence, the rows of this sparse matrix correspond to documents in the corpus, the columns represent each word in the vocabulary of the corpus, and each cell contains the TF-IDF value for a given word in a given document.\n",
    "\n",
    "Inspired by a [computer science lab exercise from Duke University](http://www.cs.duke.edu/courses/spring14/compsci290/assignments/lab02.html), the code sample below iterates through the Reuters Corpus to build a dictionary of stemmed tokens for each article, then uses the <code>TfidfVectorizer</code> and scikit-learn's own built-in stop words list to generate the term-document matrix containing TF-IDF scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    "import datetime, re, sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I'm going to create a function that takes in text and both stems and tokenizes the text. This will save us some time when we start to construct tf-idfs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_and_stem(text):\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the `reuters` corpus again. However, the data structure that it is in can be used right away. We first need to cover it from its current format into a `dictionary`. \n",
    "\n",
    "I'm going to first create an empty `dictionary` called `token_dict` and then populate it with each article from `reuters`.\n",
    "\n",
    "To access each article from `reuters` I'm going to use its own method called `.fileids()`. \n",
    "\n",
    "***Note:*** Doing this is specific to `reuters` and probably won't be encountered with anything else you do in `Python`, so `.fileids()` isn't important to know. However, I think it is important to see how data can be stored in a myriad of ways and that you need to be ready for anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_dict = {}\n",
    "for article in reuters.fileids():\n",
    "    token_dict[article] = reuters.raw(article)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have stored the text from each article as its own unique key-value pair in the dictionary, let's define our tf-idf. \n",
    "\n",
    "Thanks to `nltk` this is mostly taken care of for us with `TfidfVectorizer()`. Here, I pass in our own tokenizing function that we just defined. (There is a default one, but the one we made is much more thorough.) I also specify that we want to remove all \"English\" stopwords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=tokenize_and_stem, stop_words='english', decode_error='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can build the term-to-document matrix. All we need to do is to take the tfidf that we just defined, and pass in the text. Since we stored it as a `dictionary` we need to pass in its `.values()` to get the text. \n",
    "\n",
    "***Note***: This may take a few minutes to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tdm = tfidf.fit_transform(token_dict.values()) # this can take some time (about 60 seconds on my local machine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've built the term-document matrix, we can explore its content.\n",
    "\n",
    "First, let's get the total number of terms and documents in this corpus. We can partially do that by using the `.get_features_names()` method from the `tfidf` to extract the terms. Let's save that as `feature_names`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_names = tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the `tdm` to get the number of documents, using the `.shape[0]` to get the number of rows (i.e., documents)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('TDM contains ' + str(len(feature_names)) + ' terms and ' + str(tdm.shape[0]) + ' documents')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the term-to-document matrix has over 20,000 terms and 10,000 documents. It's quite large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's pick four terms at random. (I'm going to use the module `random` and the function `randint()` to do this.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As such, every time you run this code, it'll return something new! Try for yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(0, 4):\n",
    "    print('random term: ' + feature_names[randint(0,len(feature_names) - 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tfidf is a document specific value. Sometimes, it's helpful to know what a term's largest tfidf value is in the corpus. Let's modify the for loop above and find out what the maximum tfidf value is for 4 random terms. \n",
    "\n",
    "We can use both `feature_names` and the `tfm`. The `tfm` is the term-to-frequency matrix. It's VERY big, so it's stored in a special data format called a \"sparse matrix\" from the module `numpy`. As opposed to a `DataFrame` from `pandas`, these matrices are much more efficient with memory, but they're harder to wrangle with. \n",
    "\n",
    "Here, we're not going to go in to all of the details about how this works. Instead, let's focus on getting the tfidf values. \n",
    "\n",
    "To get all tfidf values for a term, we can retrieve it by using `tfm[:,term_index]`, where `term_index` refers to its specific \"column\" in the matrix. If this sounds confusing, don't worry, because `feature_names` matches to this index. \n",
    "\n",
    "In other words, the index of the term in `features_names` is the same as it is in `tfm`, where the terms are the columns in the matrix. To return columns with this special matrix, we need to use a `:` to return all rows in that specific column, or  `tfm[:,term_index]`, where the first element is the row index and the second element after the comma is the column index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(0, 4):\n",
    "    random_term_index = randint(0,len(feature_names) - 1)\n",
    "    random_term = feature_names[random_term_index]\n",
    "    print(\"Random term \" + random_term + \" has a maximum tfidf value of \" + str(tdm[:,random_term_index].max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could also calculate it's average tfidf score as well. (There's no one right way to do this.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "# Checkpoint 10 of 10\n",
    "## Now you try! \n",
    "\n",
    "### Using the \"State of the Union\" speech corpus, first create the term-to-document matrix. (Here, \"document\" refers to the individual speeches.) \n",
    "\n",
    "### Calculate the number of terms and the number of documents (speeches) in the corpus. \n",
    "\n",
    "### Extract the TF-IDF values for the term `congress` and report its average TF-IDF across documents. \n",
    "\n",
    "### **EXTRA CREDIT:** Calculate the average TF-IDF value for the term `american`. How does it compare to the value for `congress`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
