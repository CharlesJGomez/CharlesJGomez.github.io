{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font face=\"times\"><font size=\"6pt\"><p style = 'text-align: center;'> The City University of New York, Queens College\n",
    "\n",
    "<font face=\"times\"><font size=\"6pt\"><p style = 'text-align: center;'><b>Introduction to Computational Social Science</b><br/><br/>\n",
    "\n",
    "<p style = 'text-align: center;'><font face=\"times\"><b>Lesson 05 | SQL Databases and Multiprocessing </b><br/><br/>\n",
    "\n",
    "\n",
    "<p style = 'text-align: center;'><font face=\"times\"><b>9 Checkpoints</b><br/><br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Begin Lesson 05\n",
    "(Source: https://medium.com/analytics-vidhya/programming-with-databases-in-python-using-sqlite-4cecbef51ab9) \n",
    "(Source: https://medium.com/@urban_institute/using-multiprocessing-to-make-python-code-faster-23ea5ef996ba) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are aspiring to be a data/computational social scientist, you are going to be working with a lot of data! \n",
    "\n",
    "Much of the data reside in databases. Thus, you should be comfortable accessing data from databases through queries and then working on them to find key insights.\n",
    "\n",
    "***\n",
    "***\n",
    "\n",
    "Many forms of data exist in databases. From the number of passengers in an airport to the count of stationary in a bookshop, everything is recorded today in form of digital files called **databases**. \n",
    "\n",
    "However, databases are nothing more than electronic lists of information. \n",
    "\n",
    "Some databases are simple, and designed for smaller tasks while others are powerful, and designed for big data. \n",
    "\n",
    "All of them, however, have the same commonalities and perform a similar function. \n",
    "\n",
    "Different database tools store that information in unique ways. \n",
    "    - Flat files use a table.\n",
    "    - SQL databases use a relational model\n",
    "    - NoSQL databases use a key-value model.\n",
    "\n",
    "Here, we focus only on the **Relational Databases** and accessing them in Python. \n",
    "\n",
    "We will begin by having a quick overview of the Relational databases and their important constituents.\n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relational Databases\n",
    "\n",
    "A Relational database consists of one or more tables of information. The rows in the table are called **records** and the columns in the table are called **fields** or **attributes**. \n",
    "\n",
    "A database that contains **TWO or more related tables** is called a **relational database** \n",
    "\n",
    "The main idea behind a relational database is that your data gets broken down into common themes, with one table dedicated to describing the records of each theme.\n",
    "\n",
    "***\n",
    "\n",
    "## Database tables\n",
    "Each table in a relational database has one or more columns, and each column is assigned a specific data type, such as an integer number, a sequence of characters (for text), or a date. Each row in the table has a value for each column.\n",
    "A typical fragment of a table containing employee information may look as follows:\n",
    "\n",
    "![Database](Images/05_relationaldbexample.png)\n",
    "\n",
    "The tables of a relational database have some important characteristics:\n",
    "    - There is no significance to the order of the columns or rows.\n",
    "    - Each row contains one and only one value for each column.\n",
    "    - Each value for a given column has the same type.\n",
    "\n",
    "Each table in the database should hold information about **ONE SPECIFIC** thing only, such as employees, products, or customers.\n",
    "\n",
    "By designing a database this way, it helps to eliminate redundancy and inconsistencies. \n",
    "\n",
    "For example, both the sales and accounts payable departments may look up information about customers. In a relational database, the information about customers is entered **only once** in a table that both departments can access.\n",
    "\n",
    "***\n",
    "***\n",
    "\n",
    "## Primary and Foreign Keys\n",
    "\n",
    "We know that a relational database is a set of related tables. We use **primary and foreign keys** to describe relationships between the information in different tables. In other words, primary and foreign keys define the relational structure of a database. \n",
    "\n",
    "These keys enable each row in the database tables to be identified and define the relationships between the tables.\n",
    "\n",
    "![Database](Images/05_relationaldbexample2.png)\n",
    "\n",
    "#### Primary Key\n",
    "The primary key of a relational table uniquely identifies each record in the table. It is a **column**, or set of columns, that allows **each row in the table to be uniquely identified**. No two rows in a table with a primary key can have the same primary key value.\n",
    "\n",
    "In other words, imagine you have a **CUSTOMERS** table that contains a record for each customer visiting a store. The customer’s unique number is a good choice for a **primary key**. The customer’s first and last name **are not good** choices because there is always the chance that more than one customer might have the same name.\n",
    "\n",
    "#### Foreign Key\n",
    "A foreign key is a field in a relational table that matches the **primary key column of another table**.\n",
    "\n",
    "The example above gives a good idea of the primary and foreign keys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "# Database Management Systems and SQL\n",
    "\n",
    "The **Database management system (DBMS)** is the software that interacts with end users, applications, and the database itself to capture and analyze data. \n",
    "\n",
    "The DBMS used for **relational databases** is called Relational Database Management Systems (RDBMS). \n",
    "\n",
    "Most commercial RDBMSes use **Structured Query Language (SQL)**, a declarative language for manipulating data, to access the database. The major RDBMS are \n",
    "    - Oracle\n",
    "    - MySQL\n",
    "    - Microsoft SQL Server \n",
    "    - PostgreSQL\n",
    "    - Microsoft Access\n",
    "    - SQLite\n",
    "\n",
    "Databases are the focus and concern of data engineers, but we need to be familiar how they are set up and (more importantly) how to query from them. \n",
    "\n",
    "We will focus on using `Python` to access relational databases using a very easy to use database engine called `SQLite`. It is a relational database management system based on the SQL language but optimized for use in small environments such as mobile phones or small applications. It is self-contained, serverless, zero-configuration and transactional. It is very fast and lightweight, and the entire database is stored in a single disk file. `SQLite` is built for simplicity and speed compared to a hosted client-server relational database such as `MySQL`. It sacrifices sophistication for utility and complexity for size. \n",
    "\n",
    "However, queries in `SQLite` are almost identical to other `SQL` calls, so if you know one version of `SQL`, you essentially know them all. \n",
    "\n",
    "\n",
    "`SQLite` can be integrated with `Python` using a  module called `sqlite3`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The first thing to keep in mind is that we have to import _driver_ code - that is the API for the specific database that we want to use. The most common are:\n",
    "\n",
    "- [psycopg2](http://initd.org/psycopg/)\n",
    "- [MySQL Connector/Python](http://dev.mysql.com/doc/connector-python/en/)\n",
    "- [sqlite3](https://docs.python.org/2/library/sqlite3.html)\n",
    "\n",
    "Though a host of other databases for vendors like IBM, Microsoft, and Oracle can be found at [Python Database Interfaces](https://wiki.python.org/moin/DatabaseInterfaces). \n",
    "\n",
    "In this tutorial we will be using `sqlite3` because it ships with Python (the other drivers are third party) and because it is so simple to use. SQLite databases are the embedded backbone of many applications, though they should be kept small. \n",
    "\n",
    "## Connecting to a Database\n",
    "\n",
    "The first thing you have to do is make a connection to a database. Often times this means you'll need the following information to connect to a database server:\n",
    "\n",
    "- hostname\n",
    "- port \n",
    "- username\n",
    "- password\n",
    "- database name\n",
    "\n",
    "SQLite is an _embedded_ database, however - which means it is stored in a file on disk, and operated on soley by a single program (not multiple programs at once). Therefore in order to create a connection to a SQLite database, we simply need to point it to a file on disk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DBPATH = 'Data/people.db'\n",
    "conn = sqlite3.connect(DBPATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you should notice that a file called `people.db` has been created in your `Data/` folder! \n",
    "\n",
    "Let's check it out! Let's `cd` into `Data` (don't forget to add the `/` after `Data`) and list out the files. You should see `people.db` listed at the very bottom, and it should be empty. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%cd Data/\n",
    "%ls -ltr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, let's go back to our original working directory and double-check that we're not in `Data` anymore. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%cd ..\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now that we've created an (empty) database, let's get to work. \n",
    "\n",
    "First, the `connect` method returns a connection object that we've called `conn`. With `conn` you can manipulate your connection to the database including the following methods:\n",
    "\n",
    "- `conn.commit()` - commit any changes back to the database\n",
    "- `conn.close()` - close our connection to the database and tidy up\n",
    "\n",
    "However, to execute SQL against the database to `INSERT` or `SELECT` rows, we'll need to create a cursor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cursor is essentially a pointer into the database. Think of it like a mouse cursor that keeps track of where you are on in the database table or tables. Cursors have the following methods:\n",
    "\n",
    "- `cursor.execute()` - executes a SQL string against the database\n",
    "- `cursor.fetchone()` - fetch a single row back from the executed query\n",
    "- `cursor.fetchall()` - fetch all results back from the executed query. \n",
    "\n",
    "Together, connections and cursors are the basic way to interact with a SQL database. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## Describing the Database\n",
    "\n",
    "The first thing we have to do is describe the type of data that we'll be putting in the database by creating a _schema_. For this workshop, we'll be creating a very simple contacts application, our schema is as follows:\n",
    "\n",
    "![Contacts Schema](Images/05_schema.png)\n",
    "\n",
    "Here we have two tables, `contacts` which keeps track of people, their email, and who they are affiliated with, and `companies` which keeps tracks of organizations. To create the companies table we would execute SQL as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sql =   '''\n",
    "        CREATE TABLE IF NOT EXISTS companies (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        name TEXT NOT NULL)\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cursor.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A note on the syntax above - since I like to write clean, well-indented `SQL`. \n",
    "\n",
    "You can also use a string concatentation method in `Python`, by opening up a parentheses and adding strings _without commas_ on new lines between them. \n",
    "\n",
    "If you print `SQL` you'll see it's just one long string with spaces inside of it. You could also use docstrings with the three quotes `\"\"\"` to write a multiline string, or even read in the `SQL` from a file. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "# Checkpoint 1 of 9\n",
    "## Now you try!\n",
    "\n",
    "### We've just created the `companies` table. Now let's create the `contacts` table in the same manner. To create it, look at the schema in the image above. \n",
    "### You'll need an `id`, `name`, `email`, and `company_id` saved in various formats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## Inserting Records \n",
    "\n",
    "The next thing we'll want to do is insert some records into the database; let's add `Queens College` to the companies table.\n",
    "\n",
    "Here, we'll create a `SQL` template for inserting the names of companies into the table. Look at the query below>  \n",
    "\n",
    "We don't have to assign an id, since it will be automatically assigned using the `AUTOINCREMENT` property of that field.\n",
    "\n",
    "The `?` is a parameter to the query, and can be used as a placeholder for any user input. Values for the parameters are then passed to the second argument of the `execute` method as a tuple. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sql = \"INSERT INTO companies (name) VALUES (?)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's execute it! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cursor.execute(sql, (\"Queens College\",))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing we need is the `commit` call. \n",
    "\n",
    "_Nothing will be written to the database until commit is called_. This gives us an interesting ability to do _transactions_ - a series of `SQL` queries that when completed together succesfully, we commit them. \n",
    "\n",
    "However if something goes wrong during execution, we don't commit and therefore \"rollback\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As a side note:** You **_SHOULD NOT_** use string formatting methods like: \n",
    "\n",
    "```python\n",
    "sql = \"INSERT INTO companies (name) VALUES ({})\".format(\"Queens College\")\n",
    "```\n",
    "\n",
    "This is potentially unsafe behavior, and the `?` parameters do a lot of work on your behalf to make sure things work correctly and securely. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "# Checkpoint 2 of 9\n",
    "## Now you try! \n",
    "\n",
    "### Let's go ahead and insert another record using the same `SQL` statement. Let's add in the `NYC Department of Education`. \n",
    "### First, let's reuse the `SQL` string from before:\n",
    "\n",
    "                   sql = \"INSERT INTO companies (name) VALUES (?)\"\n",
    "\n",
    "### But now add this new organization to the table `companies`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "# Selecting Records\n",
    "\n",
    "So, let's take a step back and first learn about the basic anatomy of a `SQL` statement. Let's examine a simple query that selects a column from a sample table. Look at the image below: \n",
    "\n",
    "![SQL_Statement](Images/05_anatomy-of-select-statement.png)\n",
    "\n",
    "The basic `SQL` statement always has two parts: a `SELECT` and a `FROM`. The `SELECT` are the columns from the tables you wish to query and return. The `FROM` is where the columns in the `SELECT` are coming from. \n",
    "\n",
    "In example above, we're selecting the `id` and `username` columns from the table called `users`. \n",
    "\n",
    "We can use `SELECT *` to select all fields from a table if so desired. \n",
    "\n",
    "While not required, we can also use a `WHERE` clause! Think of it like a \"subset\" of your table, where we pick the rows from the table that meet some criterion. In our example, we're only returning rows from the columns `id` and `username` if the column `enabeled` has the value `true`.\n",
    "\n",
    "**NOTE:** One ODD feature of `SQL` is that for boolean evaluations, whereas in `Python` and most other languages we could use the double `=` (==) to see if a value is equal to another, `SQL` only uses ONE `=`. \n",
    "\n",
    "**ALSO NOTE:** While `Python` is case and indent sensitive, most `SQL` languages aren't. However the clause commands like `SELECT` OR `WHERE` DO NOT need to be capitalized or indented in a particular way for the query to work, but are captialized (and sometimes indented) out of convention. \n",
    "\n",
    "Indeed, notice that in the `SQL` statement only ONE `=` is used in the `WHERE` clause. \n",
    "\n",
    "***\n",
    "\n",
    "Let's give it a try! \n",
    "\n",
    "Let's try and query the `id` column from the `companies` table. In other words, we're going to `SELECT` `id` coming FROM the table `companies`, and let's only select those rows `WHERE` the company's name is `Queens College`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sql_query_Select = '''\n",
    "            SELECT id\n",
    "            FROM companies\n",
    "            WHERE name = \"Queens College\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cursor.execute(sql_query_Select)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's fetch the results from the query using `cursor.fetchall()` and printing it to the screen. \n",
    "\n",
    "Alternatively, you could also use the `fetchone()` statement, which goes and gets the first record it finds. Note that the name of companies are not constrained uniqueness, therefore there could be multiple \"Queens College\" records fetched from this query. \n",
    "\n",
    "If you wanted all of them, you would use `fetchall()`. Since in our table there is only one row with the name \"Queens College\" we can use either one.\n",
    "\n",
    "In either case, since \"Queens College\" was the first entry in the table, it should have an id of 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "# Checkpoint 3 of 9\n",
    "## Now you try!\n",
    "\n",
    "### Write your own `SQL` query that returns the ID of the \"`NYC Department of Education`\" from the previous checkpoint. \n",
    "### Run the query and return the results. The ID should be `2`, as it's the second value in the table `companies`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Querying Using Tuples\n",
    "\n",
    "We can also write and execute our queries in a different way, by passing the value in the `WHERE` clause as a parameter into `cursor.execute()`. \n",
    "\n",
    "Using the same parameter statement in our predicate clause and passing in the tuple containing \"Queens College\" as an argument to `execute`, we can select the id that we need, which is returned as a `Row`. \n",
    "\n",
    "`Row`s present themselves as tuples, and since we only fetched the ID, it is the first element in the record. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cursor.execute(\"SELECT id FROM companies WHERE name=?\", (\"Queens College\",))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(cursor.fetchone())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Inserting Multiple Values (Rows)\n",
    "\n",
    "Now to insert a person (me!) who works for Queens College you would write a statement similar to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sql = \"INSERT INTO contacts (name, email, company_id) VALUES (?,?,?)\" \n",
    "cursor.execute(sql, (\"Charles J. Gomez\", \"charles.gomez@qc.cuny.edu\", 1)) #ID of 1 b/c it's at QC\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "# Checkpoint 4 of 9\n",
    "## Now you try!\n",
    "\n",
    "### Using the same method above, insert some contacts into the `contacts` table and insert some new `companies` into the `companies` table. \n",
    "\n",
    "### Note that in the `contacts` table, you need to include a `company_id` for each record. Be sure to keep track of who works where! For instance, a value of `1` is \"Queens College\" and a value of `2` is the \"NYC Department of Education\".  So, you'll first need to add company values and then add contacts to keep track of who works where. \n",
    "\n",
    "### For instance, if you add a new organization, it'll have a value of `3`, and any `contact` you add who works in `3` will need to have the ID of `3`. Whereas any person you add who works at \"Queens College\" should have an ID of `1`. \n",
    "\n",
    "### Add about two to three records in each table. (Be creative!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joins with `SQL`\n",
    "\n",
    "`SQL` handles relationships through the use of `JOIN`s. `JOIN`s are clauses in `SQL` statements that link two tables based on one or more fields. \n",
    "\n",
    "With `JOINs`, relational databases can reduce redundancy. A table doesn't have to contain all the fields related to its rows, and table data can be used in multiple places. In other words, when information is stored in separate tables, the way to recombine the information is to do a join. In `SQL`, `JOIN` is a means of combining fields from two tables by using values common to each.\n",
    "\n",
    "\n",
    "There are several types of JOINs. To describe these different types, we need to add some data to our database so we can show the characteristics of each type. We commonly use Venn diagrams to represent the behavior of `SQL` joins, such as this one:   \n",
    "![SQL Joins](Images/05_sqljoins.jpg)\n",
    "\n",
    "Here are some additional resources that you may find helpful as you are learning about and practicing SQL JOINS (and using `sqlite`):\n",
    "\n",
    "\n",
    "- [Tutorials point on Joins](http://www.tutorialspoint.com/sql/sql-using-joins.htm)\n",
    "    - [Tutorials point specifically for sqlite](http://www.tutorialspoint.com/sqlite/)    \n",
    "    \n",
    "- [W3 schools on Joins](http://www.w3schools.com/sql/sql_join.asp)\n",
    "    - [W3 schools resources specifically for sqlite](http://www.w3resource.com/sqlite/index.php)    \n",
    "    \n",
    "- [A visual explanation of sql joins](http://blog.codinghorror.com/a-visual-explanation-of-sql-joins/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "Joins are just the `SQL` name for `merges` in `Pandas` (or `R`). There's nothing really too unfamiliar about them. The underlying principles are the same. \n",
    "\n",
    "Let's start with a simple `JOIN`. We have two tables: `contacts` and `companies`. They are connected by the key `company_id`. You added a few records in the previous checkpoints. So now let's join these tables together using the `company_id`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sql_join = '''\n",
    "            SELECT companies.name AS company_name,\n",
    "                    contacts.name AS contact_name,\n",
    "                    contacts.email\n",
    "            FROM companies\n",
    "            JOIN contacts\n",
    "            ON companies.id = contacts.company_id\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what this code is doing step by step. \n",
    "\n",
    "***\n",
    "\n",
    "```Python \n",
    "            SELECT companies.name AS company_name,\n",
    "                    contacts.name AS contact_name,\n",
    "```\n",
    "\n",
    "When we select columns from specific tables, we use the format `TABLE`.`COLUMN_NAME` in the query. So, since we want `name` from `companies` and `name` from `contacts` we use `companies.name` and `contacts.name` in `SELECT` to query these columns. So, our query is merely selecting from the tables `companies` and `contacts` the names of the companies and personnel we've added, respectively. \n",
    "\n",
    "**However**, it's generally a best practice for column names to be distinct. Here, `name` is the `companies` table refers to a company name, while `name` in `contacts` is a person's name. This sort of annoying naming convention will pop up a lot in data tables. \n",
    "\n",
    "Using `AS`, we can rename the columns of the resulting query. So, I re-label `name` from companies as `company_name` and `name` from `contacts` as `contact_name`. \n",
    "\n",
    "***\n",
    "\n",
    "```Python\n",
    "            contacts.email\n",
    "```\n",
    "I'm also going to select the emails of the contacts from the `contact` table. \n",
    "***NOTE*** that I do not include another comma here, as this is the last column we're selecting. \n",
    "\n",
    "***\n",
    "\n",
    "```Python\n",
    "            FROM companies\n",
    "```\n",
    "\n",
    "Next, I define my `FROM` statement. I'm going pull from the table `companies`. \n",
    "\n",
    "***\n",
    "\n",
    "```Python\n",
    "            JOIN contacts\n",
    "```\n",
    "\n",
    "Then, I'm going to `JOIN` `companies` on a new table `contacts`. (Again, this is what `SQL` calls a `merge`.) \n",
    "\n",
    "Since this is just a simple `JOIN`, I can swap `companies` and `contacts` in the `FROM` and the `JOIN` clauses, as the order doesn't matter for `JOIN` operations. **THIS IS NOT THE CASE FOR OTHER JOINS, WHERE ORDER MATTERS.**\n",
    "\n",
    "***\n",
    "```Python\n",
    "            ON companies.id = contacts.company_id\n",
    "```\n",
    "\n",
    "Finally, I need to define what the name of the keys by which we will join these tables. **NOTE** that the names of the same ID are **_DIFFERENT_** in the two tables, `id` in `companies` and `company_id` in `contacts`. Also **note** that `id` nor `company_id` are in the `SELECT` clause, as you don't need them. `SELECT` _just_ returns what you want!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cursor.execute(sql_join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what we got! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output format isn't exactly great for us humans to read. \n",
    "\n",
    "So, let's bring in `Pandas` and convert this output into a `DataFrame`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lucky for us, `Pandas` has already has this solved for us! \n",
    "\n",
    "All we have to do is use the function `pd.read_sql()` and input in our `SQL` code and the connection to the database. \n",
    "\n",
    "Here, our `SQL` code in `sql_leftjoin` and our connection is `conn`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Contact_Company_Join_df = pd.read_sql(sql_join, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Contact_Company_Join_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Way better! \n",
    "\n",
    "We'll use `Pandas` from now on to query in data. \n",
    "\n",
    "**NOTE:** We can't create tables in our `SQL` database with `pd.read_sql()`, as it merely reads in data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "# Checkpoint 5 of 9\n",
    "## Now you try! \n",
    "\n",
    "### Perform the same query as above, but name `companies.name` and `contacts.name` as something different than  `company_name` or `contact_name`. \n",
    "### In addition, add a `WHERE` clause at the end of your join query, and return rows where the contact DOES NOT work at \"Queens College\". \n",
    "### To peform a `NOT` operation, use `!=` operator. In `SQL`, you can also use its `NOT` operator `<>`  as well. \n",
    "\n",
    "### Finally, use `Pandas` and read in your query into a `DataFrame` to capture the results. \n",
    "\n",
    "### **NOTE:** What `name` (from the table `companies`) do you use in the `WHERE` clause, the full name `companies.name` or your new alias? In `sqlite`, you can use either, but not for all versions of `SQL`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "# `SQL` with `Pandas` and Complex Joins with Marvel Superheroes \n",
    "\n",
    "Now, let's do something fun! Let's explore the Marvel Superhero Universe using the concept of `SQL` `JOINS` (in particular, we'll be using `sqlite`). \n",
    "\n",
    "![Marvel Superheroes](Images/05_marvel.png) (from http://vignette1.wikia.nocookie.net/marveldatabase/images/e/e1/The_Marvel_Universe.png/revision/latest?cb=20110513164401)\n",
    "\n",
    "There are a **lot** of superheroes in the Marvel Universe, and a **lot** of superhero movies, so we'll just be examining a small portion today. \n",
    "\n",
    "As you'll see, we are storing the information about the superheroes separately from the information about the superhero movies. In addition, since superheroes frequently use aliases rather than their real names, it makes sense that we might want to store those real names separately from their superhero personae. \n",
    "\n",
    "Ok, let's get started. We've already imported `sqlite`, so we'll need to establish a connection to the database. \n",
    "\n",
    "In this case however, we've already built a database for you called \"superheroes.db\" in your `Data` folder, and it will be your job to implement different kinds of joins in order to answer the questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DBPATH_MSU = 'Data/superheroes.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn_MSU = sqlite3.connect(DBPATH_MSU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what the database looks like:\n",
    "\n",
    "![SuperheroesDB](Images/05_superheroesDB.png)\n",
    "\n",
    "Next we'll create a cursor object to help us execute our `SQL` statements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cursor_MSU = conn_MSU.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `JOIN` statements we write will all follow roughly the same pattern that our `SELECT` statements followed in this morning's workshop. That is to say they will look something like:\n",
    "\n",
    "```python\n",
    "cursor.execute(\"SELECT [information] FROM [first_table] [SOME JOIN] [second_table] ON firsttable.id = secondtable.id\")\n",
    "print(cursor.fetchall())\n",
    "```\n",
    "\n",
    "***\n",
    "***\n",
    "\n",
    "## Who's the star of the movie?\n",
    "\n",
    "How can we query our database to determine who is the star of the movie? \n",
    "\n",
    "In this case, let's use `LEFT JOIN` to pair the title of each movie with the name of its main character.\n",
    "\n",
    "Look at the query below. Here I'm going to use the `*` symbol in the `SELECT` statement to return ALL of the columns in both tables. \n",
    "\n",
    "However, when I want to select specific columns from specific tables, I again use the format `TABLE`.`COLUMN_NAME` in the `ON` clause in the query. \n",
    "\n",
    "Here, I'm selecting all columns from `Movies` and `Superheroes` using the `*`, and then I'm joining these two tables on their joint `key` column, in this case `superhero_id`. I'm doing this with the `ON` clause. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sql_leftjoin = '''\n",
    "            Select * \n",
    "                FROM Movies\n",
    "            LEFT JOIN RealNames\n",
    "            ON Movies.superhero_id = RealNames.superhero_id\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cursor_MSU.execute(sql_leftjoin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(cursor_MSU.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "The format isn't exactly great for us humans. \n",
    "\n",
    "So, let's bring in `Pandas` and convert this output into a `DataFrame`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lucky for us, `Pandas` has already has this solved for us! \n",
    "\n",
    "All we have to do is use the function `pd.read_sql()` and input in our `SQL` code and the connection to the database. \n",
    "\n",
    "Here, our `SQL` code in `sql_leftjoin` and our connection is `conn`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Marvel_df = pd.read_sql(sql_leftjoin, conn_MSU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Marvel_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, it worked! Notice that while both `superhero_id` columns from both tables are fully aligned because the `JOIN`, we now have two columns! This is both redundant (and a waste of hard drive space) and may be confusing for future joins with this new table. \n",
    "\n",
    "Futhermore, `id` is a row id of their respective tables that doesn't provide any useful information. \n",
    "\n",
    "How do we clean this up a bit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "# Checkpoint 6 of 9\n",
    "## Now you try!\n",
    "\n",
    "### Repeat the same `LEFT JOIN` merge as before, but now select specific columns from each table:\n",
    "    - title and superhero_id from the table Movies\n",
    "    - name from the table RealNames\n",
    "### Write the query in `SQL` and read it in with `Pandas`. \n",
    "\n",
    "### *Hint:* You don't need to have the column in the `SELECT` statement in order for you to do a join!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## Note on Right Joins and Full Outer Joins\n",
    "\n",
    "In `sqlite` you cannot perform them. Again, we're using `sqlite` as a pedagogical tool. The various `SQL` platforms you'll use will be very similar to `sqlite` and your queries will allow you to use both sorts of `JOINs`. Since we're restricted by CPU and memory-usage, we cannot use them here. \n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "# Nested Joins\n",
    "\n",
    "We can also run nested joins, or joins within joins. So, instead of running two joins at once, we can run and construct our output all in one large query. \n",
    "\n",
    "Let's look at our original query. \n",
    "\n",
    "```Python\n",
    "            Select * \n",
    "                FROM Movies\n",
    "            LEFT JOIN RealNames\n",
    "            ON Movies.superhero_id = RealNames.superhero_id\n",
    "```\n",
    "\n",
    "This essentially produces a table. If so, we can query from it! If so, then we can treat this query as a table and perform other query operations on it! In the query above, we `LEFT JOINed` `Movies` with `RealNames`. \n",
    "\n",
    "Say we now want to know their superhero names. We can nest this query and treat it as a table in a NEW query and join it with the superhero names. \n",
    "\n",
    "**NOTE:** The superhero id in the table `Superheroes` is just `id` and not `superhero_id` as it is in the other tables. \n",
    "\n",
    "```Python\n",
    "        Select *\n",
    "            From Superheroes\n",
    "        JOIN (\n",
    "              Select * \n",
    "                FROM Movies\n",
    "              LEFT JOIN RealNames\n",
    "              ON Movies.superhero_id = RealNames.superhero_id\n",
    "              ) AS Movies_RealNames\n",
    "        ON Movies_RealNames.superhero_id = Superheores.id\n",
    "```        \n",
    "\n",
    "This nested query looks scary at first, but look at what it just really is:\n",
    "\n",
    "```Python\n",
    "        Select *\n",
    "            From Superheroes\n",
    "        JOIN Movies_RealNames\n",
    "        ON Movies_RealNames.superhero_id = Superheores.id\n",
    "``` \n",
    "\n",
    "It's just a regular join with a new table we made! We just need to use `AS` to rename the table. Here, I called our nested query table `Movies_RealNames` as it's the merge between the tables `Movie` and `RealNames`. \n",
    "\n",
    "Let's read it in and check it out! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sql_nested = '''\n",
    "        Select *\n",
    "            From Superheroes\n",
    "        JOIN (\n",
    "              Select * \n",
    "                FROM Movies\n",
    "              LEFT JOIN RealNames\n",
    "              ON Movies.superhero_id = RealNames.superhero_id\n",
    "              ) AS Movies_RealNames\n",
    "        ON Movies_RealNames.superhero_id = Superheroes.id\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Marvel_Nested_df = pd.read_sql(sql_nested, conn_MSU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Marvel_Nested_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***\n",
    "***\n",
    "\n",
    "# Checkpoint 7 of 9\n",
    "## Now you try!\n",
    "\n",
    "### Create a run your own nested query! Don't merely copy-and-paste the one we went through here. Create your own (i.e., change the order of the joins). \n",
    "\n",
    "### Use the three tables from the Marvel Superhero Universe database. \n",
    "\n",
    "### Again, use `Pandas` to read in and store the query as a `DataFrame`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiprocessing in Python\n",
    "\n",
    "You'll likely want to speed up your code at some in your career. Thankfully multiprocessing is here to help. \n",
    "\n",
    "CPUs with multiple cores have become the standard in the recent development of modern computer architectures and we can not only find them in supercomputer facilities but also in our desktop machines at home, and our laptops!\n",
    "\n",
    "However, the default `Python` interpreter was designed with simplicity in mind and has a thread-safe mechanism: the “GIL” (Global Interpreter Lock). In order to prevent conflicts between threads, it executes only one statement at a time. This is called \"serial processing,\" or \"single-threading.\"\n",
    "\n",
    "To get around, we'll use the `multiprocessing` module and we will see how we can spawn multiple subprocesses to avoid some of the GIL’s disadvantages.\n",
    "\n",
    "The image below distinguishes between serial processing, what we've been doing up to this point, and parallel processing. \n",
    "    - Serial memory processing is the act of attending to and processing one item at a time. \n",
    "    - Parallel memory processing, by constract, is the act of attending to and processing all items simultaneously.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![serial vs multiprocessing](Images/05_multiprocessing_scheme.png)\n",
    "\n",
    "\n",
    "***\n",
    "***\n",
    "\n",
    "Another way of visualizing this is shown below for both serial and multiprocessing. \n",
    "Serial processing with the GIL is shown here: \n",
    "\n",
    "![serial vs multiprocessing](Images/05_serial_processing.jpeg)\n",
    "\n",
    "***\n",
    "***\n",
    "\n",
    "Multiprocessing that subverts the GIL is shown here: \n",
    "\n",
    "![serial vs multiprocessing](Images/05_multi_processing.jpeg)\n",
    "\n",
    "***\n",
    "***\n",
    "\n",
    "# Multi-Threading vs. Multi-Processing\n",
    "\n",
    "Depending on the application, two common approaches in parallel programming are either to run code via _threads_ or _multiple processes_. \n",
    "\n",
    "## On Threads (Shared Memory)\n",
    "\n",
    "If we submit “jobs” to different threads, those jobs can be pictured as “sub-tasks” of a single process. However, those threads will usually have access to the same memory areas (i.e., shared memory). As such, this approach can easily lead to conflicts where there is if processes are writing to the same memory location at the same time. (Like outputting to a CSV, for instance.)\n",
    "\n",
    "## Multiple Processes (Distributed Memory)\n",
    "\n",
    "A safer approach (although it comes with an additional overhead due to the communication overhead between separate processes) is to submit _multiple processes_ to completely separate memory locations (i.e., distributed memory): **Every process will run completely independent from each other.**\n",
    "\n",
    "Here, we will take a look at `Python`’s multiprocessing module and how we can use it to submit multiple processes that can run independently from each other in order to make best use of our CPU cores.\n",
    "\n",
    "***\n",
    "***\n",
    "\n",
    "# The `multiprocessing` module\n",
    "The multiprocessing module in Python’s Standard Library has a lot of powerful features. If you want to read about all the nitty-gritty tips, tricks, and details, I would recommend to use the official documentation as an entry point.\n",
    "\n",
    "Let's go through a brief overview of different approaches to show how the multiprocessing module can be used for parallel programming.\n",
    "\n",
    "## The `Process` class and the `Pool` class\n",
    "The `multiprocessing Python` module contains two classes capable of handling tasks. \n",
    "\n",
    "The `Process` class sends each task to a different processor, and the `Pool` class sends sets of tasks to different processors.\n",
    "\n",
    "While both classes provide a similar speed increase, the `Process` class is more efficient when there are not many processes to execute. Using Pythonanywhere we normally only have 4 CPUs, so this is our case. \n",
    "\n",
    "**Key Take-away:**\n",
    "`Pool` is most useful for **LARGE** number of processes and where each process can execute quickly.\n",
    "`Process` is most useful for a **SMALL** number of processes where each process takes a longer time to execute.\n",
    "\n",
    "***\n",
    "***\n",
    "\n",
    "## The `Pool` class\n",
    "The convenient approach for simple parallel processing tasks is provided by the `Pool` class.\n",
    "\n",
    "There are four methods that are particularly interesting:\n",
    "    - Pool.apply\n",
    "    - Pool.map\n",
    "    - Pool.apply_async\n",
    "    - Pool.map_async\n",
    "\n",
    "The `Pool.apply` and `Pool.map` methods are basically equivalents to `Python`’s in-built `apply` and `map` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to create some sort of function. This is called a \"worker.\" It's a function which will be executed in parallel. \n",
    "\n",
    "Here, let's define a worker (i.e., function) called `worker` that takes in one parameter. Let's make it simple and let's say it takes in some number and returns its square. \n",
    "\n",
    "**Note:** You don't need to call your function `worker`, you can call it whatever you'd like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def worker(x):\n",
    "    return x*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, how many processors are we going to use? We can first find out how many processors are available to us using the function `cpu_count()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that with `pythonanywhere.com`, you can increase the alloted number of CPUs. It should default for most of us to 4, however you can increase this to even more, if you want! You'll just need to pay a bit more per month. \n",
    "\n",
    "Okay, so let's set the number of processors to be the maximum number possible, or 1 minus `cpu_count()`. \n",
    "\n",
    "Why minus 1? If **EVERY CPU** were allocated to your process, there's nothing else left for your computer to run its most basic functions (i.e., the OS, etc.). \n",
    "\n",
    "**NEVER SET THE NUMBER OF PROCESSORS TO THE NUMBER OF CORES AVAILABLE. At most, it MUST be 1 less.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_processors = multiprocessing.cpu_count() - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create a pool of processors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p=Pool(processes = num_processors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now let's get them to work in parallel! For each worker, it will work on the function with one particular parameter. \n",
    "\n",
    "Let's create a list of parameters that we'll pass. Since our function squares the value, let's square the values for the numbers 0, 1, 2, 3, and 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = [0,1,2,3,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use the method `.map()` to map these parameters to our workers. \n",
    "\n",
    "Note that we have five parameters (0 through 4) and only three workers. That's okay! Workers will each be assigned one parameter to work on, and then when they're done and available, get the next value. In this way, it's a bit like enhanced serial processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = p.map(worker,parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's close out the pool using the method `.close()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the output. It should return a list of values, each of which are the respective squares of the inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One **BIG** caveat about using parallel processing in any form of interactive Python (Jupyter Notebooks or terminal-based iPython) is that it might not always work as we've written it here. \n",
    "\n",
    "The reasons are a bit complicated. However, if you're expecting to write more complicated you'll need to add a \n",
    "\n",
    "```python \n",
    "    if __name == '__main__'\n",
    "``` \n",
    "\n",
    "clause before calling your workers. \n",
    "\n",
    "The example below should work just as it did above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ ==  '__main__': \n",
    "    num_processors = multiprocessing.cpu_count() - 1\n",
    "    p=Pool(processes = num_processors)\n",
    "    output = p.map(worker,parameters)\n",
    "    p.close()\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "# Checkpoint 8 of 9\n",
    "## Now you try!\n",
    "\n",
    "### Write-up your own simple function (maybe it takes a number a raises it to some n-power or does something else rather basic) and use multiprocessing to print out some output, just as you did here. \n",
    "### Input a short list of a few test numbers. \n",
    "\n",
    "### **BONUS**--What would you need to do to pass in more than one set of parameters? For example, what if you wrote a function that took in both a number and the n-th power you wanted to raise that number to as parameters? **Hint:** Consider using a list of `tuples`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define our multiprocessing code as a function itself! In other words, we can pass in the parameters into a function that are then passed in to the `map()` method.\n",
    "\n",
    "Let's try it out. I'm taking the code we just wrote and putting it into its own function. **Note** that I changed `parameters` to `my_parameters` to avoid confusing a local versus a global parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multiprocessing_function(my_parameters):\n",
    "    if __name__ ==  '__main__': \n",
    "        num_processors = multiprocessing.cpu_count() - 1\n",
    "        p=Pool(processes = num_processors)\n",
    "        output = p.map(worker,my_parameters)\n",
    "        p.close()\n",
    "        return(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'm going to pass in `parameters` into our new function `multiprocessing_function` and return our original `output`. You'll see we get the same result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "multiprocessing_function(parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Performance?\n",
    "\n",
    "So, how long does this actually take?\n",
    "\n",
    "Let's try it with a 1000 numbers, instead of just five. Now, let's use the `time` module to track how long it takes with different number of processes. So far, we've used 1 minus the total number of processors. \n",
    "\n",
    "On my machine, that's 3. \n",
    "\n",
    "So, let's see how long it takes with 1, 2, and 3 dedicated processors. \n",
    "\n",
    "First, let's create a list of new parameters that goes from 0 to 999. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_parameters = [i for i in range(0,1000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run our code in a `for loop`, where we pass in different numbers of available processors the pool can use. Here we're only going to use 1, 2, or 3 CPUs. \n",
    "\n",
    "For each time we use a different number of CPUs, we're going to output how long it takes in seconds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time \n",
    "for num_worker_processors in [1,2,3]:\n",
    "    if __name__ ==  '__main__': \n",
    "        start_time = time.process_time()\n",
    "        p=Pool(processes = num_worker_processors)\n",
    "        output = p.map(worker,new_parameters)\n",
    "        end_time = time.process_time() - start_time\n",
    "        print(num_worker_processors,end_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, with only one processor, it takes about 0.01 seconds, but with two processors, it takes longer, 0.02, and even longer with three, or about 0.03. \n",
    "\n",
    "Why is this the case? Recall the discussion earlier about `pool` and `process`. \n",
    "\n",
    "***\n",
    "***\n",
    "\n",
    "# The `Process` Class\n",
    "\n",
    "Let's now turn our attention to the `process class`. Let's import it from `multiprocessing`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first create two new functions that are a bit more complex: `basic_function()` and `some_complex_func()`.\n",
    "\n",
    "With `some_complex_func()`, we square the number that was passed in and print out if it's even or odd by calling `basic_func()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def basic_func(x):\n",
    "    if x == 0:\n",
    "        return 'zero'\n",
    "    elif x%2 == 0:\n",
    "        return 'even'\n",
    "    else:\n",
    "        return 'odd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def some_complex_func(x):\n",
    "    y = x*x\n",
    "    time.sleep(2)\n",
    "    print('{} squared results in a/an {} number'.format(x, basic_func(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use the `Process` class.\n",
    "\n",
    "We've already import `Process` and created a complicated function called `some_complex_func()`. I\n",
    "\n",
    "```python\n",
    "    processes_list = []\n",
    "    for i in range(0,10):\n",
    "        p = Process(target=some_complex_func, args=(i,))\n",
    "        processes_list.append(p)\n",
    "        p.start()\n",
    "```\n",
    "\n",
    "In the first `for loop`, shown above, we create a series of `Processes`. We pass in the `target`, or our function called `some_complex_func()`. We also pass in the arguments given as a `tuple`. These are the numbers we're going to square and return if it's even or odd. We're just going to go through numbers 0 through 9. \n",
    "\n",
    "Then, we append them together in a list called `processes_list`, which we define before the `for loop`. \n",
    "\n",
    "Finally, we start them using the method `.start()`. \n",
    "\n",
    "```python\n",
    "    for process in processes_list:\n",
    "        process.join()\n",
    "```\n",
    "\n",
    "Now, we can join them together! The last loop just calls the `.join()` method on each process in the list `process_list`. This tells `Python` to wait for the process to terminate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    starttime = time.time()\n",
    "    processes_list = []\n",
    "    for i in range(0,10):\n",
    "        p = Process(target=some_complex_func, args=(i,))\n",
    "        processes_list.append(p)\n",
    "        p.start()\n",
    "        \n",
    "    for process in processes_list:\n",
    "        process.join()\n",
    "        \n",
    "    print('That took {} seconds'.format(time.time() - starttime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of this looks fine! \n",
    "\n",
    "And it only took 2 seconds! So, let's try this now with `Pool`. Below, I use the same code we went through before, but now use our new function `some_complex_func()`. Let's see how long it takes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    starttime = time.time()\n",
    "    pool = Pool(processes = num_worker_processors)\n",
    "    pool.map(some_complex_func, range(0,10))\n",
    "    pool.close()\n",
    "    print('That took {} seconds'.format(time.time() - starttime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unless you are running a machine with more than 10 processors, the `Process` code should run faster than the Pool code. \n",
    "\n",
    "We can see that's the case here: 2 seconds (`Process`) versus 8 seconds (`Pool`)\n",
    "\n",
    "This is because `Process` sends code to a processor as soon as the process is started. \n",
    "\n",
    "`Pool` instead sends a code to each available processor and doesn’t send any more until a processor has finished computing the first section of code. \n",
    "\n",
    "`Pool` does this so that processes don’t have to compete for computing resources, but this makes it slower than `Process` in cases where each process is lengthy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "# Working with Data\n",
    "\n",
    "Let's use `multiprocessing` to work with data. You can do a lot of things when you parallelize things! However, for our purposes, let's get you set up with a simple example. Reading in lots of data. \n",
    "\n",
    "Since `Pool` is better for more computational heavy tasks, we'll focus on using `Pool` rather than `Process`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "First, let's perform a simple task. \n",
    "\n",
    "Let's read in a bunch of files and combine them together into one large `DataFrame`. For simplicity, let's focus on `DataFrames` that have the same format, but are split up into different CSVs. \n",
    "\n",
    "So the function we'll use is `Pandas`'s `read_csv` and we're going to pass in as our parameter a list of the file names with their path directories. \n",
    "\n",
    "To begin, let's get the list of file names and their directories to read in. How about we read in every file in your `/Data/` directory in `/Class_Materials/` that starts with the filename `gapminder_gdp_`. Specifically, there should be five CSV files total.\n",
    "\n",
    "Let's use the module `glob` to find these filenames based on a partial `string` match. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next make sure you're in `Class_Materials`. If you're not, add a cell (of code) here and `cd` into it. Check your present working directory with `%pwd`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `glob.glob()`, we're going to go into our `Data/` folder and find the filenames and paths of all files that start with `gapminder_gdp`, where the `*` is a catch-all for anything that comes after the string. We'll learn more about it in a few lessons!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_list = glob.glob('Data/gapminder_gdp*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the contents of the list. \n",
    "\n",
    "We should see five CSV files all beginning with `gapminder_gdp`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like before, we need to put all of our desired operations into a function we'll pass in!\n",
    "\n",
    "I'm going to create a function called `my_read_csv()`, which just uses `Pandas`'s `read_csv()` function. Even if your function does only one task, it needs to be defined in a function in order for it to be mapped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_read_csv(filename):\n",
    "    return pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's have our `pool` map the file names to `DataFrames` using our new function `my_read_csv()`. \n",
    "\n",
    "Let's first set up our `pool` just as we did before and use the `.map()` method from `pool`. \n",
    "\n",
    "The only difference is this line of code:\n",
    "\n",
    "```Python\n",
    "        df_list = pool.map(my_read_csv, file_list)\n",
    "```\n",
    "\n",
    "This maps our function `my_read_csv` to the passed in parameters `file_list`, which contains the file names of each file we're reading in. \n",
    "\n",
    "(Don't forget to close your `pool`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':    \n",
    "    num_processors = multiprocessing.cpu_count() - 1\n",
    "    pool = Pool(processes = num_processors)\n",
    "    df_list = pool.map(my_read_csv, file_list)\n",
    "    pool.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what you did! Run the cell below. \n",
    "\n",
    "It's really long! Essentially, it's a `list` of `DataFrames`. Since we read in five `DataFrames` the `list` has five entries, one for each CSV. \n",
    "\n",
    "Let's check this out and verify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(df_list) #Should be five"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine these `DataFrames` together into one large `DataFrame` using `pd.concat()`. \n",
    "\n",
    "Let's see what this results with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_final = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take a peak at our new `DataFrame`, we'll see that we created one large `DataFrame`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome!\n",
    "\n",
    "While this simple example, you now have the tools to write up and perform way more complicated functions and tasks, and apply them to your data in parallel. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "# Checkpoint 9 of 9\n",
    "## Now you try!\n",
    "\n",
    "### Now, let's re-read in the same set of data, but instead of just merely reading in these files, let's add one more step. Think back to your `Pandas` training and come up with one extra step to add to our `my_read_csv()` function. It can be as simple as just returning the first 10 rows of each `DataFrame` or you can subset the `DataFrame` such that it only returns rows from a specific `Country`, one of the columns in the `DataFrame`. \n",
    "\n",
    "### Create a new function (call it whatever you want) that still reads in a CSV, but also performs one additional task. \n",
    "\n",
    "### Use `Pool` to map your function to your workers and then use `pd.concat()` to combine the results into one large `DataFrame`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
