{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font face=\"times\"><font size=\"6pt\"><p style = 'text-align: center;'> The City University of New York, Queens College\n",
    "\n",
    "<font face=\"times\"><font size=\"6pt\"><p style = 'text-align: center;'><b>Introduction to Computational Social Science</b><br/><br/>\n",
    "\n",
    "\n",
    "<p style = 'text-align: center;'><font face=\"times\"><b>Lesson 07 | Decision Trees, Random Forests, and Neural Networks</b><br/><br/>\n",
    "\n",
    "<p style = 'text-align: center;'><font face=\"times\"><b>3 Checkpoints</b><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://www.python-course.eu/neural_networks.php\n",
    "\n",
    "Source: https://www.springboard.com/blog/beginners-guide-neural-network-in-python-scikit-learn-0-18/\n",
    "\n",
    "Source: https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "# Begin Lesson 07\n",
    "## Decision Trees, Random Forests, and Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forests are an example of an *ensemble learner* built on decision trees. \n",
    "\n",
    "For this reason we'll first discuss decision trees themselves:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll explore a class of algorithms based on Decision Trees. \n",
    "\n",
    "Decision Trees encode a series of binary choices in a step-by-step process that mirrors how a person might classify things themselves. It uses an information criterion to decide which question is most useful at each step.\n",
    "\n",
    "Put simply, it's a bit like the game of \"Guess Who?\", or \"Twenty Questions\".\n",
    "\n",
    "For instance, if you wanted to create a guide to identifying an animal found in nature, you might ask the following series of questions:\n",
    "\n",
    "- Is the animal bigger or smaller than a meter long?\n",
    "    + *bigger*: does the animal have horns?\n",
    "        - *yes*: are the horns longer than ten centimeters?\n",
    "        - *no*: is the animal wearing a collar\n",
    "    + *smaller*: does the animal have two or four legs?\n",
    "        - *two*: does the animal have wings?\n",
    "        - *four*: does the animal have a bushy tail?\n",
    "\n",
    "and so on.  This binary splitting of questions is the essence of a decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Decision Tree](Images/03_decision_tree.png)\n",
    "\n",
    "Just so we get some experience with this in python, we'll import the `DecisionTreeClassifier` module from `sklearn.tree`, and create some blobs of data to try it on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `make_blobs` function is a part of sklearn.datasets.samples_generator. All methods in the package, help us to generate data samples or datasets. In machine learning, which scikit-learn all about, datasets are used to evaluate performance of machine learning models.\n",
    "\n",
    "Let's create some random data with the `make_blobs` function: let's create 300 random points, clustered into four \"blobs,\" and where each blob has some standard deviation ofa bout 0.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=300, centers=4,\n",
    "                  random_state=0, cluster_std=0.60)\n",
    "plt.figure()\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by defining a convenience function which allows us to plot the predicted value in the background:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_estimator(estimator, X, y):\n",
    "    estimator.fit(X, y)\n",
    "    x_min, x_max = X[:, 0].min() - .1, X[:, 0].max() + .1\n",
    "    y_min, y_max = X[:, 1].min() - .1, X[:, 1].max() + .1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 50),\n",
    "                         np.linspace(y_min, y_max, 50))\n",
    "    Z = estimator.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure()\n",
    "    plt.pcolormesh(xx, yy, Z, alpha=0.3)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=50)\n",
    "    plt.axis('tight')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(max_depth=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take a a look at how the decision tree did. What do you think? How did it do? What issues do you think could arise from this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_estimator(clf, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Checkpoint 1 of 3\n",
    "\n",
    "## Now you try!\n",
    "\n",
    "### We're going to use the `titanic.csv` datasets again, but now we'll train a decision tree classifier. \n",
    "\n",
    "1. First read and clean in these data, just as we did before. \n",
    "2. Subset these data to only include the data we're interested in. Here, we're going to use whether passengers survived or not (a categorical variable) as our dependent variable. We'll use other characteristics, including, class (Pclass), sex, age, and Fare as our dependent variables. \n",
    "\n",
    "### Below, I've provided some starter code for you get set up. \n",
    "### Use `GridSearchCV` to optimize on the `max_depth`, the number of layers to the tree. \n",
    "\n",
    "### Hint: for the `GridSearchCV`, the parameter space should focus on the `max_depth` as follows:\n",
    "\n",
    "```Python\n",
    "parameter_space = {\n",
    "    'max_depth': [5,10,15,20]\n",
    "}\n",
    "```\n",
    "\n",
    "### Note: Choose depth values that make sense to you! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, read in these data, using `pandas`. It's seperated in two parts: Data on the characteristics of the passengers and data on whether they surived. We need to read in both datasets as `DataFrames` and merge them on `PassengerId`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_titanic_characteristics = pd.read_csv('Data/titanic_characteristics.csv', index_col='PassengerId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_titanic_survived = pd.read_csv('Data/titanic_survived.csv',index_col='PassengerID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_titanic=pd.merge(df_titanic_characteristics,df_titanic_survived,left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset these data, where `PassengerId` is the ID of the passengers. Since we're not using it, we'll  keep it as our `DataFrame`'s index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df_titanic[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Survived']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recode sex from strings to numeric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, be sure to drop NAs from the `DataFrame`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set `Survived` as our dependent variable (i.e., our Y) and all other variables as our independent variables (i.e., our X)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_titanic = df['Survived']\n",
    "X_titanic = df.drop('Survived', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, create a generic decision tree classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_titanic = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you take it from. Recall many of the steps you took last time. It's very similar to what you need to do here. \n",
    "\n",
    "Just be sure to create a testing and training set, train the model, and optimize on the depth of the tree. To set the data, use this method in `model` to get things started.\n",
    "\n",
    "**Note**: The examples below use `X_train` and `y_train`, but they may not be labeled as such. \n",
    "\n",
    "```Python\n",
    "    model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "Use the function `accuracy_score()` and `confusion_matrix()` to see how well the model did:\n",
    "\n",
    "```Python\n",
    "    print(accuracy_score(y_test, y_predict))\n",
    "    print(confusion_matrix(y_test, y_predict))\n",
    "```\n",
    "\n",
    "What results do you get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: \n",
    "\n",
    "With graphviz installed http://www.graphviz.org/, we can export our decision tree so we can explore the decision and leaf nodes. The decision tree will look like the image above that you first saw in the lecture. \n",
    "\n",
    "To install `graphviz`, use pip3.6:\n",
    "\n",
    "What do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip3.6 install --user graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tree.export_graphviz(model_titanic, \n",
    "                     out_file='titanic_tree.dot', \n",
    "                     feature_names=X_titanic.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explore it, run the following code (hopefully it should work!). \n",
    "\n",
    "**Note**: You'll need to open the file `titanic_tree.dot` manually and see what it looks like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from subprocess import call\n",
    "\n",
    "call(['dot', '-T', 'png', 'titanic_tree.dot', '-o', 'tree.png'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees are a great tool but they can often overfit the training set of data unless pruned effectively, hindering their predictive capabilities. \n",
    "\n",
    "Indeed, a big problem with decision trees is that they can end up **over-fitting** the data. \n",
    "\n",
    "They are so flexible as models that, given a large depth, they can quickly memorize the inputs, which doesn't generalize well to previously unseen data. \n",
    "\n",
    "One way to get around this is to use many slightly different decision trees in concert. This is known as **Random Forests**, and is one of the more common techniques of **ensemble learning** (i.e. combining the results from several estimators).\n",
    "\n",
    "The \"Tree\" and \"Forest,\" a Random Forest is essentially a collection of Decision Trees. A decision tree is built on an entire dataset, using all the features/variables of interest, whereas a random forest randomly selects observations/rows and specific features/variables to build multiple decision trees from and then averages the results. After a large number of trees are built using this method, each tree \"votes\" or chooses the class, and the class receiving the most votes by a simple majority is the \"winner\" or predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=10, random_state=0)\n",
    "plot_estimator(clf, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference from the decision tree to the random forest is subtle in this example. Mainly, it seems to have gotten rid of the purple section's tail (which looked rather arbitrary from the get-go), and rounded the corners somewhat between each cluster.\n",
    "\n",
    "This helps to avoid weird classification behavior, though. Imagine there were a new data point on the left-middle section of the plot. The decision tree would assign that new data point to the purple group, even though it is plainly closer to the clusters of data points in the yellow and green sections of the plot. Using a random forest helped us avoid that kind of obvious mistake.\n",
    "\n",
    "For more complicated data the advantages can be even more obvious. Random forests are a very powerful technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Checkpoint 2 of 3\n",
    "\n",
    "## Now you try!\n",
    "\n",
    "### Repeat what you did for Checkpoint 1, but now run a Random Forest classifier. Again, use the same `titanic` data and `GridSearchCV` to optimize on the number of estimators. And again, use the method `set_model()` to set your data. \n",
    "\n",
    "### How do your results compare to just the decision tree? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "# Neural Networks\n",
    "\n",
    "When we say \"Neural Networks,\" we mean artificial Neural Networks (ANN). The idea of ANN is based on biological neural networks like the brain.\n",
    "\n",
    "Neural Networks are a machine learning framework that attempts to mimic the learning pattern of natural biological neural networks.\n",
    "\n",
    "The basic structure of a neural network is the neuron. A neuron in biology consists of three major parts: the soma (cell body), the dendrites, and the axon. The dendrites branch of from the soma in a tree-like way and getting thinner with every branch. They receive signals (impulses) from other neurons at synapses. The axon - there is always only one - also leaves the soma and usually tend to extend for longer distances than the dentrites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the above image is already an abstraction for a biologist, we can further abstract it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Neuron](Images/11_neuron_perceptron.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biological neural networks have interconnected neurons with dendrites that receive inputs, then based on these inputs they produce an output signal through an axon to another neuron. We will try to mimic this process through the use of Artificial Neural Networks (ANN), which we will just refer to as neural networks from now on. The process of creating a neural network begins with the most basic form, a single perceptron. Let's start by explaining the single perceptron.\n",
    "\n",
    "A **perceptron** of artificial neural networks is simulating the biological neuron pictured above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## The Perceptron\n",
    "\n",
    "A perceptron has one or more inputs, a bias, an activation function, and a single output. The perceptron receives inputs, multiplies them by some weight, and then passes them into an activation function to produce an output. There are many possible activation functions to choose from, such as the logistic function, a trigonometric function, a step function etc. We also make sure to add a bias to the perceptron, this avoids issues where all inputs could be equal to zero (meaning no multiplicative weight would have an effect).\n",
    "\n",
    "Once we have the output we can compare it to a known label and adjust the weights accordingly (the weights usually start off with random initialization values). We keep repeating this process until we have reached a maximum number of allowed iterations, or an acceptable error rate.\n",
    "\n",
    "To create a neural network, we simply begin to add layers of perceptrons together, creating a multi-layer perceptron model of a neural network. You'll have an input layer which directly takes in your feature inputs and an output layer which will create the resulting outputs. Any layers in between are known as hidden layers because they don't directly \"see\" the feature inputs or outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Neuron](Images/11_wiki_neural_network.png)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Neuron](Images/11_neuron_neural_network.png)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input signals get multiplied by weight values. Specifically, each input has its own corresponding weight (or weights, when it is connected to more than one node in the next layer). This way the input can be adjusted individually for every $x_{i}$. We can see all the inputs as an input vector and the corresponding weights as the weights vector.\n",
    "\n",
    "When a signal comes in--generally marked by a 1 or a 0, though it can vary depending on the application--it gets multiplied by the weight value assigned to that particular input (e.g. 1*$w_{i}$ or 0*$w_{i}$) and passed on to the node(s) in the next layer of the neural network. This can scale: if a neuron has three inputs or outputs, then it has three weights,  each of which can be adjusted individually. The weights usually get adjusted during the learning phase.\n",
    "\n",
    "After this the modified input signals are summed up. (It is also possible to add a so-called bias \"b\" to this sum. The bias is a value which can also be adjusted during the learn phase.)\n",
    "\n",
    "Finally, the actual output has to be determined. For this purpose an activation or step function Φ is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Neuron](Images/11_neuron_neural_network_detailled_view.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest form of an activation function is a binary function. If the result of the summation is greater than some threshold s, the result of Φ will be 1, otherwise 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Neuron](Images/11_Function_ANN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following image shows the general building principle of a simple artificial neural network. In short inputs pass in various numeric values. Each input has some arbitrary weight, where all the weights across the input sum to 100%. \n",
    "\n",
    "We combine these inputs and weights into some sort of weighted sum. if this weighted sum is larger than some value (this is the activation function), then this weighted sum passes as output to the next level of the network. However, if this weighted sum is less than some value, then 0 is outputed to the next level. \n",
    "\n",
    "Depending onthe number of hidden layers you have, each output is an input to another perceptron. \n",
    "\n",
    "These weights are altered during the training session, so each time the ANN is run the weights are (hopefully) improved to get better and better results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Neuron](Images/11_perceptron_activation_simple.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## Creating an \"AND\" Neural Net\n",
    "\n",
    "We will write a very simple Neural Network implementing the logical \"AND\" function. It is defined for two input, as show below. The \"AND\" function works simiarly to multiplication. \n",
    "\n",
    "For instance, if Input 1 \"AND\" Input 2 are both 0, then the output is 0. However, if Input 1 is 0 \"AND\" Input 2 is 1, the output is 0 (i.e., 1x0=0). Only when both inputs are 1 will the output be 1. \n",
    "\n",
    "This simple Neural Net setup takes in two inputs and produces one output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Neuron](Images/11_Input_Output_Table.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I wrote up a three useful functions, bundled together as a \"package\" called a class. (In many ways, packages we've already encountered like `pandas` and `scikit-learn` are \"classes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class ADD_Perceptron:\n",
    "    \n",
    "    def __init__(self, input_length, weights=None):\n",
    "        if weights is None:\n",
    "            self.weights = np.ones(input_length) * 0.5\n",
    "        else:\n",
    "            self.weights = weights\n",
    "        \n",
    "    @staticmethod\n",
    "    def unit_step_function(x):\n",
    "        if x > 0.5:\n",
    "            return 1\n",
    "        return 0\n",
    "        \n",
    "    def __call__(self, in_data):\n",
    "        weighted_input = self.weights * in_data\n",
    "        weighted_sum = weighted_input.sum()\n",
    "        return ADD_Perceptron.unit_step_function(weighted_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`def __call__` is doing the heavy lifting here. \n",
    "\n",
    "The variable `weighted_input`  multiplies the inputs by their weights. \n",
    "`weighted_sum` simply adds these results together, and the final line applies our step function, which says it should activate only if the sum is greater than 0.5. \n",
    "\n",
    "With two inputs of either `1` or `0` and with carefully chosen weights, we can get this neural network to return `1` *only* when the two inputs are *both* `1`.  \n",
    "\n",
    "So, let's use our handcoded function for a single perceptron with two inputs, and let's give each a weight of 0.5 to do just that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = ADD_Perceptron(2, np.array([0.5, 0.5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's feed the perceptron's two inputs with the values from the table, given by:\n",
    "\n",
    "- [0,0]\n",
    "- [0,1]\n",
    "- [1,0]\n",
    "- [1,1]\n",
    "\n",
    "I'm going to feed in each of these inputs one at a time (hence the for-loop) into  `p`, my perceptron class that I made. Remember, this simple perceptron models the AND function. So let's see what the output is for each of these inputs. \n",
    "\n",
    "Let's look at the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for x in [np.array([0, 0]), np.array([0, 1]), \n",
    "          np.array([1, 0]), np.array([1, 1])]:\n",
    "    y = p(np.array(x))\n",
    "    print(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output matches the table above. In other words, we've coded a very simple (and not very interesting) neural network that works like an AND function. \n",
    "\n",
    "Now, let's more to something more complicated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "# ANN with `Sci-kit Learn`\n",
    "\n",
    "Let's move on to actually creating a Neural Network with `sci-kit` learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Data\n",
    "\n",
    "For this analysis we will look at [wine fraud](https://en.wikipedia.org/wiki/Wine_fraud), which is actually a very real thing. We'll test if ANNs can help with this problem. \n",
    "\n",
    "We will use the wine data set from the UC, Irvine's Machine Learning Repository. It has various chemical features of different wines, all grown in the same region in Italy, but the data is labeled by three different possible ***cultivars***. \n",
    "\n",
    "We will try to build an ANN model that can classify what cultivar a wine belongs to based on its chemical features. (The data comes from [here.](https://archive.ics.uci.edu/ml/datasets/Wine).)\n",
    "\n",
    "First let's import the dataset, and add header columns with the names function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "wine = pd.read_csv('Data/wine_data.csv', \n",
    "                   names = [\"Cultivator\", \n",
    "                            \"Alchol\", \n",
    "                            \"Malic_Acid\", \n",
    "                            \"Ash\", \n",
    "                            \"Alcalinity_of_Ash\", \n",
    "                            \"Magnesium\", \n",
    "                            \"Total_phenols\", \n",
    "                            \"Falvanoids\", \n",
    "                            \"Nonflavanoid_phenols\", \n",
    "                            \"Proanthocyanins\", \n",
    "                            \"Color_intensity\", \n",
    "                            \"Hue\", \n",
    "                            \"OD280\", \n",
    "                            \"Proline\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wine.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should have 178 data points with 13 features and 1 label column (\"Cultivator\") for a total of 14 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wine.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up our data and our labels. Specifically, \"cultivators\" is our dependent / outcome variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use all of the other data in this dataframe, except for our y-variable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = wine.drop('Cultivator',axis=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's isolate the y-variable here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = wine['Cultivator'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at `Cultivator`. It is a categorical variable with three types.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Train Test Split\n",
    "\n",
    "Let's split our data into training and testing sets, this is done easily with SciKit Learn's train_test_split function from model_selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Data Preprocessing\n",
    "\n",
    "The neural network may have difficulty converging before the maximum number of iterations allowed if the data is not normalized. In other words, we need our variables to be comparable. \n",
    "\n",
    "Thus, before we do anything, we need to **standardize** these variables. This usually means centering these varibles (i.e., or substraing the mean of each variable for all of its data points) and then creating some common unit (i.e., usually achieved by then dividing this value by its standard deviation, so that all of your variables are comparable in terms of standard deviations). In other words, all of our variables will have mean = 0 and variance = 1. \n",
    "\n",
    "There are a lot of different methods for normalization of data, we will use the built-in StandardScaler for standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, fit the training data. We'll use `scaler` to calcualte the mean and standard deviations for each column in the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now apply these values to your training and testing data, so that all of the variables are scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check it out for yourself. The mean for all thirteen variables is 0. You'll notice that it's some super small number like, 5.018959e-17 (i.e., zero). You'll also notice that the standard deviation is 1 for each variable too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Training the Model\n",
    "\n",
    "Now it is time to train our model. SciKit Learn makes this incredibly easy, by using estimator objects. In this case we will import our estimator (the Multi-Layer Perceptron Classifier model) from the neural_network library of SciKit-Learn!\n",
    "\n",
    "If the import of `MLPClassifier` fails, you can upgrade `sklearn` by uncommenting and running the next line of code, but you probably won't need to on this platform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!pip3.6 install --user --upgrade sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create an instance of the model, there are a lot of parameters you can choose to define and customize here, we will only define the hidden_layer_sizes. For this parameter you pass in a tuple consisting of the number of neurons you want at each layer, where the nth entry in the tuple represents the number of neurons in the nth layer of the MLP model. \n",
    "\n",
    "There are many ways to choose these numbers, but for simplicity we will choose 3 hidden layers with the same number of neurons as there are features in our data set along with 500 max iterations.\n",
    "\n",
    "How does this work to set up the model? Notice that we have 13 wine features from our dataset. We are including three layers to our ANN. Hence, we pass in a tuple to `hidden_layer_sizes` as (13,13,13). \n",
    "\n",
    "This is what it would theoretically look like:\n",
    "\n",
    "![Three Layer](Images/11_three_layer_ann.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(13,13,13),max_iter=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A technical sidenote**: the choice of these numbers in practice can be somewhat complex. [This](http://www.faqs.org/faqs/ai-faq/neural-nets/part1/preamble.html) discussion gives a good run-down on the issue of layer size (and number of layers). \n",
    "\n",
    "**In general**, you have as many input nodes as you do features in your dataset, and only as many output nodes as are needed to capture your required output. You only need one output node for a neural network regressor, for instance, since it only has a single number to return. \n",
    "Categorical variables are also usually conveyed by a single node, though there are situations (specifically when you want to use a method called `softmax` to figure out those values) when you may need more. Only the most intuitive neural network designers have a good sense of how many nodes to put in the hidden layer(s). Most architectures are initialized with the number of nodes in the hidden layer set to equal the mean between the number of nodes in the input and output layers. Redundant (or un-weighty) nodes are then pruned as the network is trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model has been made we can fit the training data to our model, remember that this data has already been processed and scaled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlp.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the output that shows the default values of the other parameters in the model. \n",
    "\n",
    "I encourage you to play around with them and discover what effects they have on your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "The downside however to using a Multi-Layer Preceptron model is how difficult it is to interpret the model itself. The weights and biases won't be easily interpretable in relation to which features are important to the model itself.\n",
    "\n",
    "However, if you do want to extract the MLP weights and biases after training your model, you use its public attributes **coefs_** and **intercepts_**. \n",
    "\n",
    "**coefs_** is a list of weight matrices, where weight matrix at index i represents the weights between layer i and layer i+1. \n",
    "\n",
    "**intercepts_** is a list of bias vectors, where the vector at index i represents the bias values added to layer i+1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have four coefficients, or four lists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(mlp.coefs_) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each list, there are 13 weights, one for each of the 13 features in our wine dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(mlp.coefs_[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(mlp.intercepts_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see how we did! \n",
    "\n",
    "The final step is to make predictions on our test data. To do so, execute the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, let's look at the classification report to see how well it did in terms of precision. \n",
    "\n",
    "What results do you see here? What independent variable did the best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "# Checkpoint 3 of 3\n",
    "\n",
    "## Now you try!\n",
    "\n",
    "### Use the Titanic dataset and create your own ANN. Follow the same steps as outlined above (i.e., create a training and test set, standardize the variables, etc.). \n",
    "\n",
    "### Be sure to think about the number of input nodes and deep layers for your model. \n",
    "\n",
    "\n",
    "### Hint: The parameter space for an `ANN` is quite large! We can't cover all the nuiances here, but use this as a rough guide when deciding on the hidden layer size in the network. \n",
    "\n",
    "#### Note: Be sure to change these values so that they correspond to the `Titanic` dataset!\n",
    "\n",
    "`parameter_space = {\n",
    "    'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)]\n",
    "}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
