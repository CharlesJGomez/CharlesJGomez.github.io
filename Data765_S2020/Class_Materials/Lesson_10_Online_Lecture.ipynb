{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font face=\"times\"><font size=\"6pt\"><p style = 'text-align: center;'> The City University of New York, Queens College\n",
    "\n",
    "<font face=\"times\"><font size=\"6pt\"><p style = 'text-align: center;'><b>Introduction to Computational Social Science</b><br/><br/>\n",
    "\n",
    "<p style = 'text-align: center;'><font face=\"times\"><b>Lesson 10 | Natural Language Processing II: Web Scraping and Text Analysis </b><br/><br/>\n",
    "\n",
    "<p style = 'text-align: center;'><font face=\"times\"><b>5 Checkpoints</b><br/><br/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "# Begin Lesson 10\n",
    "# Using Text as Data\n",
    "\n",
    "Now that we covered the basics, let's see what we can really do. It's this Notebook, we're going to learn how to \n",
    "\n",
    "- Extract data from the web\n",
    "- Sentiment analysis on text and identify its part of speech\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## Extracting Text from HTML\n",
    "\n",
    "Now, we'll start with a pretty basic and commonly-faced task: extracting text content from an HTML page. Python's `urllib3` package  gives us the tools we need to fetch a web page from a given URL, but we see that the output is full of HTML markup that we don't want to deal with.\n",
    "\n",
    "First, let's install it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip3.6 install --user urllib3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it out with some code from a newssite called \"venturebeat.com\" (Although, this ought to work many different websites.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = \"http://venturebeat.com/2014/07/04/facebooks-little-social-experiment-got-you-bummed-out-get-over-it/\"\n",
    "html = urlopen(url).read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what this looks like (just a sneak-peek, so we'll only look at the first 500 characters.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "html[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That doesn't make any sense, unless you know `html`. Thankfully for us, we have options in `Python` that will help us extract useful information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "# Checkpoint 1 of 5\n",
    "## Now you try!\n",
    "\n",
    "### Pick the website so any news article (or any website with a lot of text). Use `urlopen` to read in the page's `HTML` and look at the first 500 characters. \n",
    "\n",
    "### **Note:** You'll need a stable internet connection to to this if you're not using it on the cloud. \n",
    "\n",
    "### What do you see?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Stripping-out HTML formatting\n",
    "\n",
    "Fortunately, we can use a method called `BeautifulSoup()` to get the raw text out of an HTML-formatted string. BeautifulSoup, is a Python library for pulling data out of HTML and XML files. It parses HTML content into easily-navigable nested data structure.\n",
    "\n",
    "It's still not perfect, though, since the output will contain page navigation and all kinds of other junk that we don't want, especially if our goal is to focus on the body content from a news article, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import bs4\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `BeautifulSoup` to work, just pass in the string (in this, called `html`) and specify the format of what the string represents (this case `XML`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = BeautifulSoup(html,'xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, take a look at how nicely it converted the raw text into acutal 'XML'. It still looks overwhelming and it's hard for us to interpret, but it's a step in the right direction. \n",
    "\n",
    "**Note:** It's going to be long!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "# Checkpoint 2 of 5\n",
    "## Now you try!\n",
    "\n",
    "### With the `HTML` text you extracted from the previous checkpoint, now apply `BeautifulSoup` and convert it to `xml`. \n",
    "\n",
    "### How does it compare to the `HTML` you saw before?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Identifying the Main Content\n",
    "If we just want the body content from the article, we'll need to use two additional packages. The first is a package called `Readability`, which pulls the main body content out of an HTML document and subsequently \"cleans it up.\" \n",
    "\n",
    "Using Readability and BeautifulSoup together, we can quickly get exactly the text we're looking for out of the HTML, (*mostly*) free of page navigation, comments, ads, etc. Now we're ready to start analyzing this text content.\n",
    "\n",
    "***NOTE***: In order for `readability` to work, we'll need to download the module using `pip`. This make take a few minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip3.6 install --user readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip3.6 install --user readability-lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import readability   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from readability.readability import Document #Note, we need to call it from readability.readability, a strange quirk to how this module was originally named. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the function `Document()` and pass in our string `html` and extract the summary and title of the article, using the `summary()` and `title()` methods respectively. \n",
    "\n",
    "And let's store it as `readable_article` and `readable_title`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "readable_article = Document(html).summary()\n",
    "readable_title = Document(html).title()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a peak at the summary, but let's only look at the first 500 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "readable_article[0:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has a lot of `html` code embedded around it, and a lot of unhelpful tags. Here, `BeautifulSoup` can clear out much of this from the string.\n",
    "\n",
    "Since this is converted into `xml` we need to let `BeautifulSoup` know that's the format it's in with the parameter `lxml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(readable_article,'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's print out the title and the summary of the article that we cleaned up with `BeautifulSoup`. Here, we can use the method `.text` that will extract readable text from `soup`. Let's restrict it to the first 500 characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('*** The Title is *** \\n\\\"' + readable_title + '\\\"\\n')\n",
    "print('*** The Content is *** \\n\\\"' + soup.text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it's now much easier on the eyes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "# Checkpoint 3 of 5 \n",
    "## Now you try! \n",
    "\n",
    "### Apply `readability` to the **original** `HTML` text you extracted in checkpoint 1. Print out the title and content of the webpage. \n",
    "\n",
    "### How does this compare to what you read in checkpoint 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## Part of Speech (PoS) Tagging "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now look at an example of part of speech tagging using NLTK. Looking at the part of speech for terms is helpful for a variety of purposes. For instance, in the case of sentiment analysis--which looks at whether a term is used in a positive or negative way--understanding whether the term is used as a noun, an modifier (adjective), or a verb can help us understand the rhetorical style of a particular text. \n",
    "\n",
    "Here, we're not going to rehash (or try to recall) our gradeschool grammar classes. Essentially, using `nltk`, the PoS tagger will process a sentence (as a string) and provide what it believes the part of speech is for each term. \n",
    "\n",
    "Below, I've outlined most of the tags that it will output. Hopefully, a lot of these seem familiar (e.g., noun, adjective, verb, pronoun, etc.)\n",
    "\n",
    "### Tagset\n",
    "\n",
    "    N = noun\n",
    "    NP = noun phrase\n",
    "    Adj = adjective\n",
    "    AdjP = adjective phrase\n",
    "    Adv = adverb\n",
    "    Prep = preposition\n",
    "    PP = prepositional phrase\n",
    "    Quant = quantifier\n",
    "    Ord = ordinal numeral\n",
    "    Card = cardinal numeral\tRel-Cl = relative clause\n",
    "    Rel-Pro = relative pronoun\n",
    "    V = verb\n",
    "    S = sentence\n",
    "    Det = determiner\n",
    "    Dem-Det = demonstrative determiner\n",
    "    Wh-Det = wh-determiner\n",
    "    PPron = personal pronoun\n",
    "    PoPron = possessive pronoun\n",
    "\n",
    "So, let's test it out a sample sentence: \n",
    "\n",
    "\"WASHINGTON -- In the wake of a string of abuses by New York police officers in the 1990s, Loretta E. Lynch, the top federal prosecutor in Brooklyn, spoke forcefully about the pain of a broken trust that African-Americans felt and said the responsibility for repairing generations of miscommunication and mistrust fell to law enforcement.\"\n",
    "\n",
    "Let's import `nltk` and save this sentence as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = \"WASHINGTON -- In the wake of a string of abuses by New York police officers in the 1990s, Loretta E. Lynch, the top federal prosecutor in Brooklyn, spoke forcefully about the pain of a broken trust that African-Americans felt and said the responsibility for repairing generations of miscommunication and mistrust fell to law enforcement.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to clean up this sentence. Let's use nltk's `.word_tokenize()` method. Once that's done, we can then use `nltk` to tag the part of speech of each term in this sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_sentence = nltk.pos_tag(nltk.word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what this looks like. `pos_sentence` is a list of `tuples`, so let's use a for loop and print out each term and its PoS. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for term, part_of_speech in pos_sentence:\n",
    "    print(\"Term: \" + term+ \" | Part of Speech: \" + part_of_speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are unsure of what each of these tags means, you can always use `nltk` to return what it is and examples, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltk.help.upenn_tagset('NNP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "# Checkpoint 4 of 5\n",
    "## Now you try!\n",
    "\n",
    "### Read in some sentence. It can come from anywhere! Use the part-of-speech tagger from `nltk` and a for loop to identify every term's part-fo-speech. \n",
    "\n",
    "### You can use `nltk.help.upenn_tagset()` function to identify what the part-of-speech code means (e.g., verb, adverb, pronoun, etc.). \n",
    "\n",
    "### How well did it do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## Sentiment Analysis\n",
    "\n",
    "Now that we know how to determine the PoS of a sentence, now let's turn and do some sentiment analysis using Empath (empath.stanford.edu), which is a dictionary tool that counts words in various categories (e.g., positive sentiment, negative sentiment). \n",
    "\n",
    "First, we need to import the library and create a lexicon. \n",
    "\n",
    "(**Note:** This module isn't readily available on Anaconda, so we'll import it from a file in this directory (e.g., folder) if you use Anaconda, instead for future work.)\n",
    "\n",
    "You can actually play around with it here: empath.stanford.edu\n",
    "\n",
    "We present Empath, a tool that can generate and validate new lexical categories on demand from a small set of seed terms (like “bleed” and “punch” to generate the category violence). Empath draws connotations between words and phrases by deep learning a neural embedding across more than 1.8 billion words of modern fiction. Given a small set of seed words that characterize a category, Empath uses its neural embedding to discover new related terms, then validates the category with a crowd-powered filter. \n",
    "\n",
    "Empath can generate new lexical categories and analyze text over 200 built-in human-validated categories. \n",
    "\n",
    "First, let's install `empath` and then import it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip3 install --user empath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import empath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from empath import Empath\n",
    "lexicon = Empath()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start analyzing a sentence. \n",
    "\n",
    "With setting normalize to True, the counts are normalized according to sentence length. Here, let's tokenize the sentence as we did last week using the `nltk` method called `.word_tokenize()`.\n",
    "\n",
    "Let's test it out with this sentence:\n",
    "\n",
    "\"Bullshit, you can't even post FACTS on this sub- like Clinton lying about sniper fire.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentiment_dictionary = lexicon.analyze(nltk.word_tokenize(\"Bullshit, you can't even post FACTS on this sub- like Clinton lying about sniper fire.\"), normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's go through this sentiment_dictionary and just look at the values that are greater than zero. We can use a list to extract the values that are greater than zero, as shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[(k,v) for k,v in sentiment_dictionary.items() if v > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad. It picked up on words like \"fire,\" \"sub,\" \"lying\" to associate with \"social media,\" \"deception,\" and \"weapon.\" \n",
    "\n",
    "Let's try it out with another sentence:\n",
    "\n",
    "\"Totally agree. Planning to beat your opponent is not a sign of corruption. That's politics. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentiment_dictionary = lexicon.analyze(nltk.word_tokenize(\"Totally agree. Planning to beat your opponent is not a sign of corruption. That's politics. \"), normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[(k,v) for k,v in sentiment_dictionary.items() if v > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "# Checkpoint 5 of 5\n",
    "\n",
    "## Now you try!\n",
    "\n",
    "### Let's explore the tool with some more examples. What happens in cases of sarcasm, negation, or very informal text?\n",
    "\n",
    "### Identify three sentences: One that is sarcastic, one that negates, and an informal text with slang. \n",
    "\n",
    "### Repeat the same steps as above with your three sentences. \n",
    "\n",
    "### What categories do you pick up? Which are the top categories for each sentence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
